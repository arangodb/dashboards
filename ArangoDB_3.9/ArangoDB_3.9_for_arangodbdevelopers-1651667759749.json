{
  "annotations": {
    "list": [
      {
        "builtIn": 1,
        "datasource": {
          "type": "grafana",
          "uid": "-- Grafana --"
        },
        "enable": true,
        "hide": true,
        "iconColor": "rgba(0, 211, 255, 1)",
        "name": "Annotations & Alerts",
        "target": {
          "limit": 100,
          "matchAny": false,
          "tags": [],
          "type": "dashboard"
        },
        "type": "dashboard"
      }
    ]
  },
  "editable": true,
  "fiscalYearStartMonth": 0,
  "graphTooltip": 0,
  "id": 19,
  "links": [],
  "liveNow": false,
  "panels": [
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 0
      },
      "id": 1,
      "panels": [],
      "title": "Health",
      "type": "row"
    },
    {
      "aliasColors": {},
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total number of drop-follower events. This metric is increased on leaders\nwhenever a write operation cannot be replicated to a follower during\nsynchronous replication, and it would be unsafe in terms of data consistency\nto keep that follower.\nThis metric was named `arangodb_dropped_followers_count` in previous\nversions of ArangoDB.\n",
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1
      },
      "hiddenSeries": false,
      "id": 2,
      "legend": {
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 1,
      "nullPointMode": "null",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "8.5.1",
      "pointradius": 2,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "expr": "rate(arangodb_dropped_followers_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "thresholds": [],
      "timeRegions": [],
      "title": "Number of drop-follower events",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "mode": "time",
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "short",
          "logBase": 1,
          "show": true
        },
        {
          "format": "short",
          "logBase": 1,
          "show": true
        }
      ],
      "yaxis": {
        "align": false
      }
    },
    {
      "aliasColors": {},
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total number of failed heartbeat transmissions.\nServers in a cluster periodically send their heartbeats to\nthe Agency to report their own liveliness. This counter gets\nincreased whenever sending such a heartbeat fails. In the single\nserver, this counter is only used in active failover mode.\n",
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1
      },
      "hiddenSeries": false,
      "id": 3,
      "legend": {
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 1,
      "nullPointMode": "null",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "8.5.1",
      "pointradius": 2,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "expr": "rate(arangodb_heartbeat_failures_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "thresholds": [],
      "timeRegions": [],
      "title": "Total number of failed heartbeat transmissions",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "mode": "time",
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "short",
          "logBase": 1,
          "show": true
        },
        {
          "format": "short",
          "logBase": 1,
          "show": true
        }
      ],
      "yaxis": {
        "align": false
      }
    },
    {
      "cards": {},
      "color": {
        "cardColor": "#b4ff00",
        "colorScale": "sqrt",
        "colorScheme": "interpolateOranges",
        "exponent": 0.5,
        "mode": "spectrum"
      },
      "dataFormat": "timeseries",
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of times required to send heartbeats. For every heartbeat\nsent the time is measured and an event is put into the histogram.\nIn the single server, this counter is only used in active failover mode.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 9
      },
      "heatmap": {},
      "hideZeroBuckets": false,
      "highlightCards": true,
      "id": 4,
      "legend": {
        "show": false
      },
      "reverseYBuckets": false,
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(arangodb_heartbeat_send_time_msec_bucket[3m])) by (le))",
          "format": "heatmap",
          "legendFormat": "",
          "refId": "A"
        }
      ],
      "title": "Time required to send a heartbeat",
      "tooltip": {
        "show": true,
        "showHistogram": false
      },
      "type": "heatmap",
      "xAxis": {
        "show": true
      },
      "yAxis": {
        "format": "short",
        "logBase": 1,
        "show": true
      },
      "yBucketBound": "auto"
    },
    {
      "aliasColors": {},
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of times required to send heartbeats. For every heartbeat\nsent the time is measured and an event is put into the histogram.\nIn the single server, this counter is only used in active failover mode.\n",
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 9
      },
      "hiddenSeries": false,
      "id": 5,
      "legend": {
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 1,
      "nullPointMode": "null",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "8.5.1",
      "pointradius": 2,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "expr": "rate(arangodb_heartbeat_send_time_msec_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "thresholds": [],
      "timeRegions": [],
      "title": "Time required to send a heartbeat (count of events per second)",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "mode": "time",
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "short",
          "logBase": 1,
          "show": true
        },
        {
          "format": "short",
          "logBase": 1,
          "show": true
        }
      ],
      "yaxis": {
        "align": false
      }
    },
    {
      "aliasColors": {},
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of times required to send heartbeats. For every heartbeat\nsent the time is measured and an event is put into the histogram.\nIn the single server, this counter is only used in active failover mode.\n",
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 17
      },
      "hiddenSeries": false,
      "id": 6,
      "legend": {
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 1,
      "nullPointMode": "null",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "8.5.1",
      "pointradius": 2,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "expr": "rate(arangodb_heartbeat_send_time_msec_sum[3m]) / rate(arangodb_heartbeat_send_time_msec_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "thresholds": [],
      "timeRegions": [],
      "title": "Time required to send a heartbeat (average per second)",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "mode": "time",
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "short",
          "logBase": 1,
          "show": true
        },
        {
          "format": "short",
          "logBase": 1,
          "show": true
        }
      ],
      "yaxis": {
        "align": false
      }
    },
    {
      "aliasColors": {},
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This counter is increased whenever the io heartbeat encounters a delay\nof at least 1s when writing a small file to the database directory,\nreading it and then removing it again.\nThis test is done periodically to ensure that the underlying volume is\nusable and performs reasonably well. The test can be switched off\nexplicitly with the flag `--database.io-heartbeat=false`, but the\ndefault is `true`. Furthermore, every such failure leads to a line in\nthe log at INFO level for the `ENGINES` topic.\n",
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 17
      },
      "hiddenSeries": false,
      "id": 7,
      "legend": {
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 1,
      "nullPointMode": "null",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "8.5.1",
      "pointradius": 2,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "expr": "rate(arangodb_ioheartbeat_delays_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "thresholds": [],
      "timeRegions": [],
      "title": "Number of delays in the io heartbeat test",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "mode": "time",
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "short",
          "logBase": 1,
          "show": true
        },
        {
          "format": "short",
          "logBase": 1,
          "show": true
        }
      ],
      "yaxis": {
        "align": false
      }
    },
    {
      "cards": {},
      "color": {
        "cardColor": "#b4ff00",
        "colorScale": "sqrt",
        "colorScheme": "interpolateOranges",
        "exponent": 0.5,
        "mode": "spectrum"
      },
      "dataFormat": "timeseries",
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This histogram is updated whenever the io heartbeat runs its test in\nthe database directory. It writes a small file, syncs it to durable\nstorage, reads it, and then unlinks the file again. This test is done\nperiodically to ensure that the underlying volume is usable and performs\nreasonably well. The test can be switched off explicitly with the flag\n`--database.io-heartbeat=false`, but the default is `true`.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 25
      },
      "heatmap": {},
      "hideZeroBuckets": false,
      "highlightCards": true,
      "id": 8,
      "legend": {
        "show": false
      },
      "reverseYBuckets": false,
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(arangodb_ioheartbeat_duration_bucket[3m])) by (le))",
          "format": "heatmap",
          "legendFormat": "",
          "refId": "A"
        }
      ],
      "title": "Histogram of execution times of a single IO heartbeat check",
      "tooltip": {
        "show": true,
        "showHistogram": false
      },
      "type": "heatmap",
      "xAxis": {
        "show": true
      },
      "yAxis": {
        "format": "short",
        "logBase": 1,
        "show": true
      },
      "yBucketBound": "auto"
    },
    {
      "aliasColors": {},
      "bars": false,
      "dashLength": 10,
      "dashes": false,
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This histogram is updated whenever the io heartbeat runs its test in\nthe database directory. It writes a small file, syncs it to durable\nstorage, reads it, and then unlinks the file again. This test is done\nperiodically to ensure that the underlying volume is usable and performs\nreasonably well. The test can be switched off explicitly with the flag\n`--database.io-heartbeat=false`, but the default is `true`.\n",
      "fill": 1,
      "fillGradient": 0,
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 25
      },
      "hiddenSeries": false,
      "id": 9,
      "legend": {
        "avg": false,
        "current": false,
        "max": false,
        "min": false,
        "show": true,
        "total": false,
        "values": false
      },
      "lines": true,
      "linewidth": 1,
      "nullPointMode": "null",
      "options": {
        "alertThreshold": true
      },
      "percentage": false,
      "pluginVersion": "8.5.1",
      "pointradius": 2,
      "points": false,
      "renderer": "flot",
      "seriesOverrides": [],
      "spaceLength": 10,
      "stack": false,
      "steppedLine": false,
      "targets": [
        {
          "expr": "rate(arangodb_ioheartbeat_duration_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "thresholds": [],
      "timeRegions": [],
      "title": "Histogram of execution times of a single IO heartbeat check (count of events per second)",
      "tooltip": {
        "shared": true,
        "sort": 0,
        "value_type": "individual"
      },
      "type": "graph",
      "xaxis": {
        "mode": "time",
        "show": true,
        "values": []
      },
      "yaxes": [
        {
          "format": "short",
          "logBase": 1,
          "show": true
        },
        {
          "format": "short",
          "logBase": 1,
          "show": true
        }
      ],
      "yaxis": {
        "align": false
      }
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This histogram is updated whenever the io heartbeat runs its test in\nthe database directory. It writes a small file, syncs it to durable\nstorage, reads it, and then unlinks the file again. This test is done\nperiodically to ensure that the underlying volume is usable and performs\nreasonably well. The test can be switched off explicitly with the flag\n`--database.io-heartbeat=false`, but the default is `true`.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 33
      },
      "id": 10,
      "targets": [
        {
          "expr": "rate(arangodb_ioheartbeat_duration_sum[3m]) / rate(arangodb_ioheartbeat_duration_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Histogram of execution times of a single IO heartbeat check (average per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This counter is increased whenever the io heartbeat encounters a problem\nwhen writing a small file to the database directory, reading it and then\nremoving it again. This test is done\nperiodically to ensure that the underlying volume is usable. The test can\nbe switched off explicitly with the flag `--database.io-heartbeat=false`,\nbut the default is `true`. Furthermore, every such failure leads to a\nline in the log at INFO level for the `ENGINES` topic.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 33
      },
      "id": 11,
      "targets": [
        {
          "expr": "rate(arangodb_ioheartbeat_failures_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of failures in the io heartbeat test",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 41
      },
      "id": 12,
      "panels": [],
      "title": "AQL",
      "type": "row"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total number of AQL queries finished.\nThis metric was named `arangodb_aql_all_query` in previous\nversions of ArangoDB.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 42
      },
      "id": 13,
      "targets": [
        {
          "expr": "rate(arangodb_aql_all_query_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total number of AQL queries finished",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Current number of AQL queries executing.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 42
      },
      "id": 14,
      "targets": [
        {
          "expr": "arangodb_aql_current_query",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Current number of AQL queries executing",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total memory limit for all AQL queries combined, in bytes.\nIf this value is reported as `0`, it means there is no total memory\nlimit in place for AQL queries. The value can be adjusted by the setting\nthe `--query.global-memory-limit` startup option.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 50
      },
      "id": 15,
      "targets": [
        {
          "expr": "arangodb_aql_global_memory_limit",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total memory limit for all AQL queries combined",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total memory usage of all AQL queries currently executing.\nThe granularity of this metric is steps of 32768 bytes. The current\nmemory usage of all AQL queries will be compared against the configured\nlimit in the `--query.global-memory-limit` startup option.\nIf the startup option has a value of `0`, then no global memory limit\nwill be enforced. If the startup option has a non-zero value, queries\nwill be aborted once the total query memory usage goes above the configured\nlimit.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 50
      },
      "id": 16,
      "targets": [
        {
          "expr": "arangodb_aql_global_memory_usage",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total memory usage of all AQL queries executing; granularity: 32768 bytes steps",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total number of times the global query memory limit threshold was reached.\nThis can happen if all running AQL queries in total try to use more memory than\nconfigured via the `--query.global-memory-limit` startup option.\nEvery time this counter will increase, an AQL query will have aborted with a\n\"resource limit exceeded\" error.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 58
      },
      "id": 17,
      "targets": [
        {
          "expr": "rate(arangodb_aql_global_query_memory_limit_reached_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of times the global query memory limit threshold was reached",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total number of times a local query memory limit threshold was reached, i.e.\na single query tried to allocate more memory than configured in the query's\n`memoryLimit` attribute or the value configured via the startup option\n`--query.memory-limit`.\nEvery time this counter will increase, an AQL query will have aborted with a\n\"resource limit exceeded\" error.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 58
      },
      "id": 18,
      "targets": [
        {
          "expr": "rate(arangodb_aql_local_query_memory_limit_reached_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of times a local query memory limit threshold was reached",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Execution time histogram for all AQL queries, in seconds.\nThe histogram includes all slow queries.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 66
      },
      "id": 19,
      "legend": {
        "show": false
      },
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(arangodb_aql_query_time_bucket[3m])) by (le))",
          "format": "heatmap",
          "legendFormat": "",
          "refId": "A"
        }
      ],
      "title": "Execution time histogram for all AQL queries",
      "type": "heatmap"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Execution time histogram for all AQL queries, in seconds.\nThe histogram includes all slow queries.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 66
      },
      "id": 20,
      "targets": [
        {
          "expr": "rate(arangodb_aql_query_time_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Execution time histogram for all AQL queries (count of events per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Execution time histogram for all AQL queries, in seconds.\nThe histogram includes all slow queries.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 74
      },
      "id": 21,
      "targets": [
        {
          "expr": "rate(arangodb_aql_query_time_sum[3m]) / rate(arangodb_aql_query_time_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Execution time histogram for all AQL queries (average per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Execution time histogram for slow AQL queries, in seconds.\nQueries are considered \"slow\" if their execution time is above the\nthreshold configured in the startup options `--query.slow-threshold`\nor `--query.slow-streaming-threshold`, resp.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 74
      },
      "id": 22,
      "legend": {
        "show": false
      },
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(arangodb_aql_slow_query_time_bucket[3m])) by (le))",
          "format": "heatmap",
          "legendFormat": "",
          "refId": "A"
        }
      ],
      "title": "Execution time histogram for slow AQL queries",
      "type": "heatmap"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Execution time histogram for slow AQL queries, in seconds.\nQueries are considered \"slow\" if their execution time is above the\nthreshold configured in the startup options `--query.slow-threshold`\nor `--query.slow-streaming-threshold`, resp.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 82
      },
      "id": 23,
      "targets": [
        {
          "expr": "rate(arangodb_aql_slow_query_time_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Execution time histogram for slow AQL queries (count of events per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Execution time histogram for slow AQL queries, in seconds.\nQueries are considered \"slow\" if their execution time is above the\nthreshold configured in the startup options `--query.slow-threshold`\nor `--query.slow-streaming-threshold`, resp.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 82
      },
      "id": 24,
      "targets": [
        {
          "expr": "rate(arangodb_aql_slow_query_time_sum[3m]) / rate(arangodb_aql_slow_query_time_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Execution time histogram for slow AQL queries (average per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total execution time of all AQL queries, in milliseconds,\nincluding all slow queries.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 90
      },
      "id": 25,
      "targets": [
        {
          "expr": "rate(arangodb_aql_total_query_time_msec_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total execution time of all AQL queries",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 98
      },
      "id": 26,
      "panels": [],
      "title": "Transactions",
      "type": "row"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total amount of time it took to acquire collection/shard locks for\nwrite operations, summed up for all collections/shards. Will not be increased\nfor any read operations.\nThe value is measured in microseconds.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 99
      },
      "id": 27,
      "targets": [
        {
          "expr": "rate(arangodb_collection_lock_acquisition_micros_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total amount of collection lock acquisition time",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Number of transactions using sequential locking of collections to avoid deadlocking.\nBy default, a Coordinator will try to lock all shards of a collection in parallel.\nThis approach is normally fast but can cause deadlocks with other transactions that\nlock the same shards in a different order. In case such a deadlock is detected, the\nCoordinator will abort the lock round and start a new one that locks all shards in\nsequential order. This will avoid deadlocks, but has a higher setup overhead.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 99
      },
      "id": 28,
      "targets": [
        {
          "expr": "rate(arangodb_collection_lock_sequential_mode_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of transactions using sequential locking of collections to avoid deadlocking",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Number of timeouts when trying to acquire collection exclusive locks.\nThis counter will be increased whenever an exclusive collection lock\ncannot be acquired within the configured lock timeout.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 107
      },
      "id": 29,
      "targets": [
        {
          "expr": "rate(arangodb_collection_lock_timeouts_exclusive_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of timeouts when trying to acquire collection exclusive locks",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Number of timeouts when trying to acquire collection write locks.\nThis counter will be increased whenever a collection write lock\ncannot be acquired within the configured lock timeout.\nThis can only happen if writes on a collection are locked out by\nother operations on the collection that use an exclusive lock. Writes\nare not locked out by other, non-exclusively locked writes.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 107
      },
      "id": 30,
      "targets": [
        {
          "expr": "rate(arangodb_collection_lock_timeouts_write_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of timeouts when trying to acquire collection write locks",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total time spent in collection truncate operations, including both\nuser-initiated truncate operations and truncate operations\nexecuted by the synchronous replication on followers.\nNote that this metric is only present when the command\nline option `--server.export-read-write-metrics` is set to `true`.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 115
      },
      "id": 31,
      "legend": {
        "show": false
      },
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(arangodb_collection_truncate_time_bucket[3m])) by (le))",
          "format": "heatmap",
          "legendFormat": "",
          "refId": "A"
        }
      ],
      "title": "Total time spent in collection truncate operations",
      "type": "heatmap"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total time spent in collection truncate operations, including both\nuser-initiated truncate operations and truncate operations\nexecuted by the synchronous replication on followers.\nNote that this metric is only present when the command\nline option `--server.export-read-write-metrics` is set to `true`.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 115
      },
      "id": 32,
      "targets": [
        {
          "expr": "rate(arangodb_collection_truncate_time_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total time spent in collection truncate operations (count of events per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total time spent in collection truncate operations, including both\nuser-initiated truncate operations and truncate operations\nexecuted by the synchronous replication on followers.\nNote that this metric is only present when the command\nline option `--server.export-read-write-metrics` is set to `true`.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 123
      },
      "id": 33,
      "targets": [
        {
          "expr": "rate(arangodb_collection_truncate_time_sum[3m]) / rate(arangodb_collection_truncate_time_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total time spent in collection truncate operations (average per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total number of collection truncate operations on leaders (excluding synchronous\nreplication). Note that this metric is only present when the command\nline option `--server.export-read-write-metrics` is set to `true`.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 123
      },
      "id": 34,
      "targets": [
        {
          "expr": "rate(arangodb_collection_truncates_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total number of collection truncate operations (excluding synchronous replication)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total time spent in document insert operations, including both\nuser-initiated insert operations and insert operations executed by\nthe synchronous replication on followers. This metric\nis only present if the option `--server.export-read-write-metrics` is set\nto `true`.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 131
      },
      "id": 35,
      "legend": {
        "show": false
      },
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(arangodb_document_insert_time_bucket[3m])) by (le))",
          "format": "heatmap",
          "legendFormat": "",
          "refId": "A"
        }
      ],
      "title": "Total time spent in document insert operations",
      "type": "heatmap"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total time spent in document insert operations, including both\nuser-initiated insert operations and insert operations executed by\nthe synchronous replication on followers. This metric\nis only present if the option `--server.export-read-write-metrics` is set\nto `true`.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 131
      },
      "id": 36,
      "targets": [
        {
          "expr": "rate(arangodb_document_insert_time_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total time spent in document insert operations (count of events per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total time spent in document insert operations, including both\nuser-initiated insert operations and insert operations executed by\nthe synchronous replication on followers. This metric\nis only present if the option `--server.export-read-write-metrics` is set\nto `true`.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 139
      },
      "id": 37,
      "targets": [
        {
          "expr": "rate(arangodb_document_insert_time_sum[3m]) / rate(arangodb_document_insert_time_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total time spent in document insert operations (average per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total time spent in document read-by-primary-key operations. This metric\nis only present if the option `--server.export-read-write-metrics` is set\nto `true`.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 139
      },
      "id": 38,
      "legend": {
        "show": false
      },
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(arangodb_document_read_time_bucket[3m])) by (le))",
          "format": "heatmap",
          "legendFormat": "",
          "refId": "A"
        }
      ],
      "title": "Total time spent in document read-by-primary-key operations",
      "type": "heatmap"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total time spent in document read-by-primary-key operations. This metric\nis only present if the option `--server.export-read-write-metrics` is set\nto `true`.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 147
      },
      "id": 39,
      "targets": [
        {
          "expr": "rate(arangodb_document_read_time_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total time spent in document read-by-primary-key operations (count of events per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total time spent in document read-by-primary-key operations. This metric\nis only present if the option `--server.export-read-write-metrics` is set\nto `true`.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 147
      },
      "id": 40,
      "targets": [
        {
          "expr": "rate(arangodb_document_read_time_sum[3m]) / rate(arangodb_document_read_time_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total time spent in document read-by-primary-key operations (average per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total time spent in document replace operations, including both\nuser-initiated replace operations and replace operations executed by\nthe synchronous replication on followers. This metric\nis only present if the option `--server.export-read-write-metrics` is set\nto `true`.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 155
      },
      "id": 41,
      "legend": {
        "show": false
      },
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(arangodb_document_remove_time_bucket[3m])) by (le))",
          "format": "heatmap",
          "legendFormat": "",
          "refId": "A"
        }
      ],
      "title": "Total time spent in document remove operations",
      "type": "heatmap"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total time spent in document replace operations, including both\nuser-initiated replace operations and replace operations executed by\nthe synchronous replication on followers. This metric\nis only present if the option `--server.export-read-write-metrics` is set\nto `true`.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 155
      },
      "id": 42,
      "targets": [
        {
          "expr": "rate(arangodb_document_remove_time_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total time spent in document remove operations (count of events per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total time spent in document replace operations, including both\nuser-initiated replace operations and replace operations executed by\nthe synchronous replication on followers. This metric\nis only present if the option `--server.export-read-write-metrics` is set\nto `true`.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 163
      },
      "id": 43,
      "targets": [
        {
          "expr": "rate(arangodb_document_remove_time_sum[3m]) / rate(arangodb_document_remove_time_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total time spent in document remove operations (average per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total time spent in document replace operations, including both\nuser-initiated replace operations and replace operations executed by\nthe synchronous replication on followers. This metric\nis only present if the option `--server.export-read-write-metrics` is set\nto `true`.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 163
      },
      "id": 44,
      "legend": {
        "show": false
      },
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(arangodb_document_replace_time_bucket[3m])) by (le))",
          "format": "heatmap",
          "legendFormat": "",
          "refId": "A"
        }
      ],
      "title": "Total time spent in document replace operations",
      "type": "heatmap"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total time spent in document replace operations, including both\nuser-initiated replace operations and replace operations executed by\nthe synchronous replication on followers. This metric\nis only present if the option `--server.export-read-write-metrics` is set\nto `true`.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 171
      },
      "id": 45,
      "targets": [
        {
          "expr": "rate(arangodb_document_replace_time_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total time spent in document replace operations (count of events per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total time spent in document replace operations, including both\nuser-initiated replace operations and replace operations executed by\nthe synchronous replication on followers. This metric\nis only present if the option `--server.export-read-write-metrics` is set\nto `true`.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 171
      },
      "id": 46,
      "targets": [
        {
          "expr": "rate(arangodb_document_replace_time_sum[3m]) / rate(arangodb_document_replace_time_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total time spent in document replace operations (average per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total time spent in document update operations, including both\nuser-initiated update operations and update operations executed by\nthe synchronous replication on followers. This metric\nis only present if the option `--server.export-read-write-metrics` is set\nto `true`.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 179
      },
      "id": 47,
      "legend": {
        "show": false
      },
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(arangodb_document_update_time_bucket[3m])) by (le))",
          "format": "heatmap",
          "legendFormat": "",
          "refId": "A"
        }
      ],
      "title": "Total time spent in document update operations",
      "type": "heatmap"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total time spent in document update operations, including both\nuser-initiated update operations and update operations executed by\nthe synchronous replication on followers. This metric\nis only present if the option `--server.export-read-write-metrics` is set\nto `true`.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 179
      },
      "id": 48,
      "targets": [
        {
          "expr": "rate(arangodb_document_update_time_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total time spent in document update operations (count of events per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total time spent in document update operations, including both\nuser-initiated update operations and update operations executed by\nthe synchronous replication on followers. This metric\nis only present if the option `--server.export-read-write-metrics` is set\nto `true`.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 187
      },
      "id": 49,
      "targets": [
        {
          "expr": "rate(arangodb_document_update_time_sum[3m]) / rate(arangodb_document_update_time_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total time spent in document update operations (average per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total number of document write operations (insert, update, replace, remove) on\nleaders, excluding writes by the synchronous replication on followers.\nThis metric is only present if the option `--server.export-read-write-metrics`\nis set to `true`.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 187
      },
      "id": 50,
      "targets": [
        {
          "expr": "rate(arangodb_document_writes_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total number of document write operations (excluding synchronous replication)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total number of read-only transactions. In the cluster, this metric will\nbe collected separately for transactions on Coordinators and the\ntransaction counterparts on leaders and followers.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 195
      },
      "id": 51,
      "targets": [
        {
          "expr": "rate(arangodb_read_transactions_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of read transactions",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total number of transactions aborted. In the cluster, this metric will\nbe collected separately for transactions on Coordinators and the\ntransaction counterparts on leaders and followers.\nThis metric was named `arangodb_transactions_aborted` in previous\nversions of ArangoDB.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 195
      },
      "id": 52,
      "targets": [
        {
          "expr": "rate(arangodb_transactions_aborted_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of transactions aborted",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total number of transactions committed. In the cluster, this metric will\nbe collected separately for transactions on Coordinators and the\ntransaction counterparts on leaders and followers.\nThis metric was named `arangodb_transactions_committed` in previous\nversions of ArangoDB.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 203
      },
      "id": 53,
      "targets": [
        {
          "expr": "rate(arangodb_transactions_committed_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of transactions committed",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total number of expired transactions, i.e. transactions that have\nbeen begun but that were automatically garbage-collected due to\ninactivity within the transactions' time-to-live (TTL) period.\nIn the cluster, this metric will be collected separately for transactions\non Coordinators and the transaction counterparts on leaders and followers.\nThis metric was named `arangodb_transactions_expired` in previous\nversions of ArangoDB.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 203
      },
      "id": 54,
      "targets": [
        {
          "expr": "rate(arangodb_transactions_expired_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total number of expired transactions",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total number of transactions started/begun. In the cluster, this metric will\nbe collected separately for transactions on Coordinators and the\ntransaction counterparts on leaders and followers.\nThis metric was named `arangodb_transactions_started` in previous\nversions of ArangoDB.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 211
      },
      "id": 55,
      "targets": [
        {
          "expr": "rate(arangodb_transactions_started_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of transactions started",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of the received request sizes in bytes.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 211
      },
      "id": 58,
      "targets": [
        {
          "expr": "rate(arangodb_client_connection_statistics_bytes_received_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Bytes received for a request (count of events per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 219
      },
      "id": 56,
      "panels": [],
      "title": "Statistics",
      "type": "row"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of the received request sizes in bytes.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 220
      },
      "id": 57,
      "legend": {
        "show": false
      },
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(arangodb_client_connection_statistics_bytes_received_bucket[3m])) by (le))",
          "format": "heatmap",
          "legendFormat": "",
          "refId": "A"
        }
      ],
      "title": "Bytes received for a request",
      "type": "heatmap"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of the sent response sizes in bytes\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 220
      },
      "id": 60,
      "legend": {
        "show": false
      },
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(arangodb_client_connection_statistics_bytes_sent_bucket[3m])) by (le))",
          "format": "heatmap",
          "legendFormat": "",
          "refId": "A"
        }
      ],
      "title": "Bytes sent for a request",
      "type": "heatmap"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of the received request sizes in bytes.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 228
      },
      "id": 59,
      "targets": [
        {
          "expr": "rate(arangodb_client_connection_statistics_bytes_received_sum[3m]) / rate(arangodb_client_connection_statistics_bytes_received_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Bytes received for a request (average per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of the sent response sizes in bytes\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 228
      },
      "id": 62,
      "targets": [
        {
          "expr": "rate(arangodb_client_connection_statistics_bytes_sent_sum[3m]) / rate(arangodb_client_connection_statistics_bytes_sent_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Bytes sent for a request (average per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of the sent response sizes in bytes\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 236
      },
      "id": 61,
      "targets": [
        {
          "expr": "rate(arangodb_client_connection_statistics_bytes_sent_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Bytes sent for a request (count of events per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of the connection's total lifetime, i.e., the time between the\npoint when the connection was established until it was closed. Smaller\nnumbers indicate that there is not a lot of load and/or that connections\nare not reused for multiple requests. Consider using keep-alive header\nor HTTP/2 or VST.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 236
      },
      "id": 64,
      "legend": {
        "show": false
      },
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(arangodb_client_connection_statistics_connection_time_bucket[3m])) by (le))",
          "format": "heatmap",
          "legendFormat": "",
          "refId": "A"
        }
      ],
      "title": "Total connection time of a client",
      "type": "heatmap"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "The number of client connections that are currently open.\nNote: this metric considers only HTTP and HTTP/2 connections, but not VST\nconnections.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 244
      },
      "id": 63,
      "targets": [
        {
          "expr": "arangodb_client_connection_statistics_client_connections",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "The number of client connections that are currently open",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of the connection's total lifetime, i.e., the time between the\npoint when the connection was established until it was closed. Smaller\nnumbers indicate that there is not a lot of load and/or that connections\nare not reused for multiple requests. Consider using keep-alive header\nor HTTP/2 or VST.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 244
      },
      "id": 66,
      "targets": [
        {
          "expr": "rate(arangodb_client_connection_statistics_connection_time_sum[3m]) / rate(arangodb_client_connection_statistics_connection_time_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total connection time of a client (average per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of the connection's total lifetime, i.e., the time between the\npoint when the connection was established until it was closed. Smaller\nnumbers indicate that there is not a lot of load and/or that connections\nare not reused for multiple requests. Consider using keep-alive header\nor HTTP/2 or VST.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 252
      },
      "id": 65,
      "targets": [
        {
          "expr": "rate(arangodb_client_connection_statistics_connection_time_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total connection time of a client (count of events per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of I/O times needed to answer a request. This includes the time\nrequired to read the incoming request as well as the time required to send\nthe response.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 252
      },
      "id": 68,
      "targets": [
        {
          "expr": "rate(arangodb_client_connection_statistics_io_time_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "I/O time needed to answer a request (count of events per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of I/O times needed to answer a request. This includes the time\nrequired to read the incoming request as well as the time required to send\nthe response.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 260
      },
      "id": 67,
      "legend": {
        "show": false
      },
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(arangodb_client_connection_statistics_io_time_bucket[3m])) by (le))",
          "format": "heatmap",
          "legendFormat": "",
          "refId": "A"
        }
      ],
      "title": "I/O time needed to answer a request",
      "type": "heatmap"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of the time requests are spending on a queue waiting to be\nprocessed. The overwhelming majority of these times should be clearly\nsub-second.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 260
      },
      "id": 70,
      "legend": {
        "show": false
      },
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(arangodb_client_connection_statistics_queue_time_bucket[3m])) by (le))",
          "format": "heatmap",
          "legendFormat": "",
          "refId": "A"
        }
      ],
      "title": "Queueing time needed for requests",
      "type": "heatmap"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of I/O times needed to answer a request. This includes the time\nrequired to read the incoming request as well as the time required to send\nthe response.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 268
      },
      "id": 69,
      "targets": [
        {
          "expr": "rate(arangodb_client_connection_statistics_io_time_sum[3m]) / rate(arangodb_client_connection_statistics_io_time_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "I/O time needed to answer a request (average per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of the time requests are spending on a queue waiting to be\nprocessed. The overwhelming majority of these times should be clearly\nsub-second.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 268
      },
      "id": 72,
      "targets": [
        {
          "expr": "rate(arangodb_client_connection_statistics_queue_time_sum[3m]) / rate(arangodb_client_connection_statistics_queue_time_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Queueing time needed for requests (average per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of the time requests are spending on a queue waiting to be\nprocessed. The overwhelming majority of these times should be clearly\nsub-second.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 276
      },
      "id": 71,
      "targets": [
        {
          "expr": "rate(arangodb_client_connection_statistics_queue_time_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Queueing time needed for requests (count of events per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of the time required to actually process a request. This does not\ninclude the time required to read the incoming request, the time the request\nis sitting on the queue, or the time required to send the response.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 276
      },
      "id": 74,
      "targets": [
        {
          "expr": "rate(arangodb_client_connection_statistics_request_time_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Request time needed to answer a request (count of events per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of the time required to actually process a request. This does not\ninclude the time required to read the incoming request, the time the request\nis sitting on the queue, or the time required to send the response.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 284
      },
      "id": 73,
      "legend": {
        "show": false
      },
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(arangodb_client_connection_statistics_request_time_bucket[3m])) by (le))",
          "format": "heatmap",
          "legendFormat": "",
          "refId": "A"
        }
      ],
      "title": "Request time needed to answer a request",
      "type": "heatmap"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of the total times required to process a request. This includes\nthe time required to read the incoming request, the time the request is\nsitting in the queue, the time to actually process the request, and the\ntime required to send the response.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 284
      },
      "id": 76,
      "legend": {
        "show": false
      },
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(arangodb_client_connection_statistics_total_time_bucket[3m])) by (le))",
          "format": "heatmap",
          "legendFormat": "",
          "refId": "A"
        }
      ],
      "title": "Total time needed to answer a request",
      "type": "heatmap"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of the time required to actually process a request. This does not\ninclude the time required to read the incoming request, the time the request\nis sitting on the queue, or the time required to send the response.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 292
      },
      "id": 75,
      "targets": [
        {
          "expr": "rate(arangodb_client_connection_statistics_request_time_sum[3m]) / rate(arangodb_client_connection_statistics_request_time_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Request time needed to answer a request (average per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of the total times required to process a request. This includes\nthe time required to read the incoming request, the time the request is\nsitting in the queue, the time to actually process the request, and the\ntime required to send the response.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 292
      },
      "id": 78,
      "targets": [
        {
          "expr": "rate(arangodb_client_connection_statistics_total_time_sum[3m]) / rate(arangodb_client_connection_statistics_total_time_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total time needed to answer a request (average per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of the total times required to process a request. This includes\nthe time required to read the incoming request, the time the request is\nsitting in the queue, the time to actually process the request, and the\ntime required to send the response.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 300
      },
      "id": 77,
      "targets": [
        {
          "expr": "rate(arangodb_client_connection_statistics_total_time_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total time needed to answer a request (count of events per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This counter reflects the total number of HTTP (or VST) **DELETE**\nrequests which have hit this particular instance of `arangod`.\n\nNote that this counter is ever growing during the lifetime of the\n`arangod` process. However, when the process is restarted, it starts\nfrom scratch. In the Grafana dashboards, it is usually visualized as a\nrate per second, averaged with a sliding window of a minute.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 300
      },
      "id": 80,
      "targets": [
        {
          "expr": "rate(arangodb_http_request_statistics_http_delete_requests_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of HTTP DELETE requests",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This counter reflects the total number of **asynchronous** HTTP (or VST)\nrequests which have hit this particular instance of `arangod`. Asynchronous\nrefers to the fact that the response is not sent with the HTTP response,\nbut is rather queried separately using the `/_api/jobs` API.\n\nNote that this counter is ever growing during the lifetime of the\n`arangod` process. However, when the process is restarted, it starts\nfrom scratch. In the Grafana dashboards, it is usually visualized as a\nrate per second, averaged with a sliding window of a minute.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 308
      },
      "id": 79,
      "targets": [
        {
          "expr": "rate(arangodb_http_request_statistics_async_requests_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of asynchronously executed HTTP requests",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This counter reflects the total number of HTTP (or VST) **HEAD**\nrequests which have hit this particular instance of `arangod`.\n\nNote that this counter is ever growing during the lifetime of the\n`arangod` process. However, when the process is restarted, it starts\nfrom scratch. In the Grafana dashboards, it is usually visualized as a\nrate per second, averaged with a sliding window of a minute.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 308
      },
      "id": 82,
      "targets": [
        {
          "expr": "rate(arangodb_http_request_statistics_http_head_requests_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of HTTP HEAD requests",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This counter reflects the total number of HTTP (or VST) **GET**\nrequests which have hit this particular instance of `arangod`.\n\nNote that this counter is ever growing during the lifetime of the\n`arangod` process. However, when the process is restarted, it starts\nfrom scratch. In the Grafana dashboards, it is usually visualized as a\nrate per second, averaged with a sliding window of a minute.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 316
      },
      "id": 81,
      "targets": [
        {
          "expr": "rate(arangodb_http_request_statistics_http_get_requests_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of HTTP GET requests",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This counter reflects the total number of HTTP (or VST) **PATCH**\nrequests which have hit this particular instance of `arangod`.\n\nNote that this counter is ever growing during the lifetime of the\n`arangod` process. However, when the process is restarted, it starts\nfrom scratch. In the Grafana dashboards, it is usually visualized as a\nrate per second, averaged with a sliding window of a minute.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 316
      },
      "id": 84,
      "targets": [
        {
          "expr": "rate(arangodb_http_request_statistics_http_patch_requests_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of HTTP PATCH requests",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This counter reflects the total number of HTTP (or VST) **OPTIONS**\nrequests which have hit this particular instance of `arangod`.\n\nNote that this counter is ever growing during the lifetime of the\n`arangod` process. However, when the process is restarted, it starts\nfrom scratch. In the Grafana dashboards, it is usually visualized as a\nrate per second, averaged with a sliding window of a minute.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 324
      },
      "id": 83,
      "targets": [
        {
          "expr": "rate(arangodb_http_request_statistics_http_options_requests_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of HTTP OPTIONS requests",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This counter reflects the total number of HTTP (or VST) **PUT**\nrequests which have hit this particular instance of `arangod`.\n\nNote that this counter is ever growing during the lifetime of the\n`arangod` process. However, when the process is restarted, it starts\nfrom scratch. In the Grafana dashboards, it is usually visualized as a\nrate per second, averaged with a sliding window of a minute.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 324
      },
      "id": 86,
      "targets": [
        {
          "expr": "rate(arangodb_http_request_statistics_http_put_requests_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of HTTP PUT requests",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This counter reflects the total number of HTTP (or VST) **POST**\nrequests which have hit this particular instance of `arangod`.\n\nNote that this counter is ever growing during the lifetime of the\n`arangod` process. However, when the process is restarted, it starts\nfrom scratch. In the Grafana dashboards, it is usually visualized as a\nrate per second, averaged with a sliding window of a minute.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 332
      },
      "id": 85,
      "targets": [
        {
          "expr": "rate(arangodb_http_request_statistics_http_post_requests_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of HTTP POST requests",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This counter reflects the total number of HTTP (or VST)\nrequests that have been authenticated with the JWT superuser token,\nwhich have hit this particular instance of\n`arangod`.\n\nNote that this counter is ever growing during the lifetime of the\n`arangod` process. However, when the process is restarted, it starts\nfrom scratch. In the Grafana dashboards, it is usually visualized as a\nrate per second, averaged with a sliding window of a minute.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 332
      },
      "id": 88,
      "targets": [
        {
          "expr": "rate(arangodb_http_request_statistics_superuser_requests_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total number of HTTP requests executed by superuser/JWT",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This counter reflects the total number of HTTP (or VST) **other**\nor **ILLEGAL** requests which have hit this particular instance of\n`arangod`. These are all requests, which are not one of the following:\n`DELETE`, `GET`, `HEAD`, `POST`, `PUT`, `OPTIONS`, `PATCH`.\n\nNote that this counter is ever growing during the lifetime of the\n`arangod` process. However, when the process is restarted, it starts\nfrom scratch. In the Grafana dashboards, it is usually visualized as a\nrate per second, averaged with a sliding window of a minute.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 340
      },
      "id": 87,
      "targets": [
        {
          "expr": "rate(arangodb_http_request_statistics_other_http_requests_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of other HTTP requests",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This counter reflects the total number of HTTP (or VST) requests\nthat have been authenticated for some user (as opposed to with the\nJWT superuser token), which have hit this particular instance of\n`arangod`.\n\nNote that this counter is ever growing during the lifetime of the\n`arangod` process. However, when the process is restarted, it starts\nfrom scratch. In the Grafana dashboards, it is usually visualized as a\nrate per second, averaged with a sliding window of a minute.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 340
      },
      "id": 90,
      "targets": [
        {
          "expr": "rate(arangodb_http_request_statistics_user_requests_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total number of HTTP requests executed by user clients",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This counter reflects the total number of HTTP (or VST) requests which\nhave hit this particular instance of `arangod`. Note that this counter\nis ever growing during the lifetime of the `arangod` process. However,\nwhen the process is restarted, it starts from scratch. In the Grafana\ndashboards, it is usually visualized as a rate per second, averaged\nwith a sliding window of a minute.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 348
      },
      "id": 89,
      "targets": [
        {
          "expr": "rate(arangodb_http_request_statistics_total_requests_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total number of HTTP requests",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "On Windows, this figure contains the total number of page faults.\nOn other system, this figure contains the number of major faults the\nprocess has made which have required loading a memory page from disk.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 348
      },
      "id": 92,
      "targets": [
        {
          "expr": "rate(arangodb_process_statistics_major_page_faults_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of major page faults",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Number of intermediate commits performed in transactions.\nAn intermediate commit happens if a logical transaction needs to be\nsplit into multiple physical transaction because of the volume of data\nhandled in the transaction. The thresholds for when to perform an\nintermediate commit can be controlled by startup options\n`--rocksdb.intermediate-commit-count` (number of write operations after\nwhich an intermediate commit is triggered) and `--rocksdb.intermediate-commit-size`\n(cumulated size of write operations after which an intermediate commit is triggered).\nThe values can also be overridden for individual transactions.\nThis metric was named `arangodb_intermediate_commits` in previous\nversions of ArangoDb.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 356
      },
      "id": 91,
      "targets": [
        {
          "expr": "rate(arangodb_intermediate_commits_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of intermediate commits performed in transactions",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Number of threads in the arangod process.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 356
      },
      "id": 94,
      "targets": [
        {
          "expr": "arangodb_process_statistics_number_of_threads",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of threads",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "The number of minor faults the process has made which have not required\nloading a memory page from disk. This figure is not reported on Windows.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 364
      },
      "id": 93,
      "targets": [
        {
          "expr": "arangodb_process_statistics_minor_page_faults_total",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of minor page faults",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "The relative size of the number of pages the process has in real\nmemory compared to system memory. This is just the pages which count\ntoward text, data, or stack space. This does not include pages which\nhave not been demand-loaded in, or which are swapped out. The value is a\nratio between 0.00 and 1.00.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 364
      },
      "id": 96,
      "targets": [
        {
          "expr": "arangodb_process_statistics_resident_set_size_percent",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Resident set size as fraction of system memory",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "The total size of the number of pages the process has in real memory.\nThis is just the pages which count toward text, data, or stack space.\nThis does not include pages which have not been demand-loaded in, or\nwhich are swapped out. The resident set size is reported in bytes.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 372
      },
      "id": 95,
      "targets": [
        {
          "expr": "arangodb_process_statistics_resident_set_size",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Resident set size",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Amount of time that this process has been scheduled in user mode,\nmeasured in seconds.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 372
      },
      "id": 98,
      "targets": [
        {
          "expr": "arangodb_process_statistics_user_time",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Process user time",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Amount of time that this process has been scheduled in kernel mode,\nmeasured in seconds.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 380
      },
      "id": 97,
      "targets": [
        {
          "expr": "arangodb_process_statistics_system_time",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Process system time",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of the body sizes of the received HTTP/1.1 requests in bytes.\nNote that this does not account for the header.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 380
      },
      "id": 100,
      "legend": {
        "show": false
      },
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(arangodb_request_body_size_http1_bucket[3m])) by (le))",
          "format": "heatmap",
          "legendFormat": "",
          "refId": "A"
        }
      ],
      "title": "Body size in bytes for HTTP/1.1 requests",
      "type": "heatmap"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "On Windows, this figure contains the total amount of memory that the\nmemory manager has committed for the arangod process. On other systems,\nthis figure contains the size of the virtual memory the process is\nusing.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 388
      },
      "id": 99,
      "targets": [
        {
          "expr": "arangodb_process_statistics_virtual_memory_size",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Virtual memory size",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of the body sizes of the received HTTP/1.1 requests in bytes.\nNote that this does not account for the header.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 388
      },
      "id": 102,
      "targets": [
        {
          "expr": "rate(arangodb_request_body_size_http1_sum[3m]) / rate(arangodb_request_body_size_http1_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Body size in bytes for HTTP/1.1 requests (average per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of the body sizes of the received HTTP/1.1 requests in bytes.\nNote that this does not account for the header.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 396
      },
      "id": 101,
      "targets": [
        {
          "expr": "rate(arangodb_request_body_size_http1_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Body size in bytes for HTTP/1.1 requests (count of events per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of the body sizes of the received HTTP/2 requests in bytes.\nNote that this does not account for the header.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 396
      },
      "id": 104,
      "targets": [
        {
          "expr": "rate(arangodb_request_body_size_http2_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Body size in bytes for HTTP/2 requests (count of events per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of the body sizes of the received HTTP/2 requests in bytes.\nNote that this does not account for the header.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 404
      },
      "id": 103,
      "legend": {
        "show": false
      },
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(arangodb_request_body_size_http2_bucket[3m])) by (le))",
          "format": "heatmap",
          "legendFormat": "",
          "refId": "A"
        }
      ],
      "title": "Body size in bytes for HTTP/2 requests",
      "type": "heatmap"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of the body sizes of the received VST requests in bytes.\nNote that this does include the binary header.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 404
      },
      "id": 106,
      "legend": {
        "show": false
      },
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(arangodb_request_body_size_vst_bucket[3m])) by (le))",
          "format": "heatmap",
          "legendFormat": "",
          "refId": "A"
        }
      ],
      "title": "Body size in bytes for VST requests",
      "type": "heatmap"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of the body sizes of the received HTTP/2 requests in bytes.\nNote that this does not account for the header.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 412
      },
      "id": 105,
      "targets": [
        {
          "expr": "rate(arangodb_request_body_size_http2_sum[3m]) / rate(arangodb_request_body_size_http2_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Body size in bytes for HTTP/2 requests (average per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of the body sizes of the received VST requests in bytes.\nNote that this does include the binary header.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 412
      },
      "id": 108,
      "targets": [
        {
          "expr": "rate(arangodb_request_body_size_vst_sum[3m]) / rate(arangodb_request_body_size_vst_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Body size in bytes for VST requests (average per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of the body sizes of the received VST requests in bytes.\nNote that this does include the binary header.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 420
      },
      "id": 107,
      "targets": [
        {
          "expr": "rate(arangodb_request_body_size_vst_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Body size in bytes for VST requests (count of events per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Percentage of time that the system CPUs have been idle, as\na value between 0 and 100, and as reported by the operating system.\nThis metric is only reported on some operating systems.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 420
      },
      "id": 110,
      "targets": [
        {
          "expr": "arangodb_server_statistics_idle_percent",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Percentage of time that the system CPUs have been idle",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Number of CPU cores visible to the arangod process, unless the\nenvironment variable `ARANGODB_OVERRIDE_DETECTED_NUMBER_OF_CORES`\nis set. In that case, the environment variable's value will be reported.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 428
      },
      "id": 109,
      "targets": [
        {
          "expr": "arangodb_server_statistics_cpu_cores",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of CPU cores visible to the arangod process",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Physical memory of the system in bytes, as reported by the operating system\nunless the environment variable `ARANGODB_OVERRIDE_DETECTED_TOTAL_MEMORY`\nis set. In that case, the environment variable's value will be reported.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 428
      },
      "id": 112,
      "targets": [
        {
          "expr": "arangodb_server_statistics_physical_memory",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Physical memory in bytes",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Percentage of time that the system CPUs have been waiting for I/O, as\na value between 0 and 100, and as reported by the operating system.\nThis metric is only reported on some operating systems.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 436
      },
      "id": 111,
      "targets": [
        {
          "expr": "arangodb_server_statistics_iowait_percent",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Percentage of time that the system CPUs have been waiting for I/O",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Percentage of time that the system CPUs have spent in kernel mode, as\na value between 0 and 100, and as reported by the operating system.\nThis metric is only reported on some operating systems.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 436
      },
      "id": 114,
      "targets": [
        {
          "expr": "arangodb_server_statistics_system_percent",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Percentage of time that the system CPUs have spent in kernel mode",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Number of seconds elapsed since server start, including fractional\nseconds.\nThis metric was named `arangodb_server_statistics_server_uptime`\nin previous versions of ArangoDB.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 444
      },
      "id": 113,
      "targets": [
        {
          "expr": "rate(arangodb_server_statistics_server_uptime_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of seconds elapsed since server start",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Number of V8 contexts currently alive. Normally, only Coordinators and\nsingle servers should have V8 contexts, for DB-Servers and Agents the\nvalue is always zero.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 444
      },
      "id": 116,
      "targets": [
        {
          "expr": "arangodb_v8_context_alive",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of V8 contexts currently alive",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Percentage of time that the system CPUs have spent in user mode, as\na value between 0 and 100, and as reported by the operating system.\nThis metric is only reported on some operating systems.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 452
      },
      "id": 115,
      "targets": [
        {
          "expr": "arangodb_server_statistics_user_percent",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Percentage of time that the system CPUs have spent in user mode",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This gauge reflects the number of V8 contexts that are currently dirty.\nA V8 context is dirty, if it has executed JavaScript for some time and\nis due for a garbage collection.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 452
      },
      "id": 118,
      "targets": [
        {
          "expr": "arangodb_v8_context_dirty",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of V8 contexts currently dirty",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Number of V8 contexts currently busy, that means, they are currently\nworking on some JavaScript task. Normally, only Coordinators and\nsingle servers should have V8 contexts, for DB-Servers and Agents the\nvalue is always zero.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 460
      },
      "id": 117,
      "targets": [
        {
          "expr": "arangodb_v8_context_busy",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of V8 contexts currently busy",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This is the maximum number of concurrent V8 contexts. This is limited\nby a server option, since V8 contexts can use a lot of RAM. V8 contexts\nare created and destroyed as needed up to the limit shown in this metric.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 460
      },
      "id": 120,
      "targets": [
        {
          "expr": "arangodb_v8_context_max",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Maximum number of concurrent V8 contexts",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This gauge reflects the number of V8 contexts that are currently free.\nIf this number drops to 0 there might be a shortage of V8 contexts.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 468
      },
      "id": 119,
      "targets": [
        {
          "expr": "arangodb_v8_context_free",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of V8 contexts currently free",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total number of document write operations by synchronous replication.\nThis metric is only present if the option\n`--server.export-read-write-metrics` is set to `true`.\nTotal number of document write operations (insert, update, replace, remove)\nexecuted by the synchronous replication on followers.\nThis metric is only present if the option `--server.export-read-write-metrics`\nis set to `true`.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 468
      },
      "id": 124,
      "targets": [
        {
          "expr": "rate(arangodb_document_writes_replication_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total number of document write operations by synchronous replication",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This is the minimum number of concurrent V8 contexts. This is limited\nby a server option. V8 contexts are created and destroyed as needed\nbut there are never fewer than the limit shown in this metric.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 476
      },
      "id": 121,
      "targets": [
        {
          "expr": "arangodb_v8_context_min",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Minimum number of concurrent V8 contexts",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "When using a DC-2-DC configuration of ArangoDB this metric is active on both data-centers.\nIt indicates that the follower data-center periodically matches the available databases and collections\nin order to mirror them. If no DC-2-DC is set up this value is expected to be 0.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 476
      },
      "id": 126,
      "targets": [
        {
          "expr": "rate(arangodb_replication_cluster_inventory_requests_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "(DC-2-DC only) Number of times the database and collection overviews have been requested",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 484
      },
      "id": 122,
      "panels": [],
      "title": "Replication",
      "type": "row"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total number of collection truncate operations by synchronous\nreplication on followers. Note that this metric is only present when the command\nline option `--server.export-read-write-metrics` is set to `true`.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 485
      },
      "id": 123,
      "targets": [
        {
          "expr": "rate(arangodb_collection_truncates_replication_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total number of collection truncate operations by synchronous replication",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "During initial replication the existing data from the leader is copied asynchronously\nover to new shards. The amount of requests required to transport data to this server,\nas a replica for a shard, is counted here.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 485
      },
      "id": 128,
      "targets": [
        {
          "expr": "rate(arangodb_replication_dump_bytes_received_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total number of bytes replicated in initial asynchronous phase",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Number of refusal answers from a follower during synchronous replication.\nA refusal answer will only be sent by a follower if the follower is under\nthe impression that the replication request was not sent by the current\nshard leader. This can happen if replication requests to the follower are\ndelayed or the follower is slow to process incoming requests and there was\na leader change for the shard.\nIf such a refusal answer is received by the shard leader, it will drop the\nfollower from the list of followers.\nThis metrics was named `arangodb_refused_followers_count` in previous\nversions of ArangoDB.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 493
      },
      "id": 125,
      "targets": [
        {
          "expr": "rate(arangodb_refused_followers_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of refusal answers from a follower during synchronous replication",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "During initial replication the existing data from the leader is copied asynchronously\nover to new shards. The accumulated time the follower waited for the leader to send\nthe data is counted here.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 493
      },
      "id": 130,
      "targets": [
        {
          "expr": "rate(arangodb_replication_dump_request_time_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Accumulated wait time for replication requests in initial asynchronous phase",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Measures the time required to clone the existing leader copy of the data onto a new replica shard.\nWill only be measured on the follower server. This time is expected to increase whenever new followers\nare created, e.g. increasing replication factor, shard redistribution.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 501
      },
      "id": 127,
      "targets": [
        {
          "expr": "rate(arangodb_replication_dump_apply_time_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Accumulated time needed to apply asynchronously replicated data on initial synchronization of shards",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "During initial replication the existing data from the leader is copied asynchronously\nover to new shards. Whenever there is a communication issue between the follower and\nthe leader of the shard it will be counted here for the follower. This communication\nissues cover failed connections or http errors, but they also cover invalid or\nunexpected data formats received on the follower.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 501
      },
      "id": 132,
      "targets": [
        {
          "expr": "rate(arangodb_replication_failed_connects_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of failed connection attempts and response errors during initial\nasynchronous replication",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "During initial replication the existing data from the leader is copied asynchronously\nover to new shards. The amount of documents transported to this server, as a replica for\na shard, is counted here.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 509
      },
      "id": 129,
      "targets": [
        {
          "expr": "rate(arangodb_replication_dump_documents_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total number of documents replicated in initial asynchronous phase",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This counter exhibits the accumulated wait time for requesting actual\ndocuments for the initial replication, in milliseconds. This is part\nof the older (pre 3.8) initial replication protocol, which might\nstill be used in 3.8 for collections which have been created by older\nversions.\n\nIn this older protocol, the follower first fetches an overview over\na shard from the leader. This does a full collection scan and\ndivides the primary keys in the collection into equal sized chunks.\nThen, a checksum for each chunk is returned. The same is then done\non the follower and the checksums are compared, chunk by chunk. For\neach chunk, for which the checksums do not match, the list of keys and\nrevisions is fetched from the leader. This then enables the follower\nto fetch the actually needed documents and remove superfluous ones\nlocally.\n\nThis metric accumulates the time used for the final step of actually\ngetting the needed documents.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 509
      },
      "id": 134,
      "targets": [
        {
          "expr": "rate(arangodb_replication_initial_docs_requests_time_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Accumulated time needed to request replication docs data",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "During initial replication the existing data from the leader is copied asynchronously\nover to new shards. The amount of data transported to this server, as a replica for\na shard, is counted here.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 517
      },
      "id": 131,
      "targets": [
        {
          "expr": "rate(arangodb_replication_dump_requests_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of requests used in initial asynchronous replication phase",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This counter exhibits the accumulated wait time for fetching key\nlists for a chunk, in milliseconds. This is part of the\nolder (pre 3.8) initial replication protocol, which might still be used\nin 3.8 for collections which have been created by older versions.\n\nIn this older protocol, the follower first fetches an overview over\na shard from the leader. This does a full collection scan and\ndivides the primary keys in the collection into equal sized chunks.\nThen, a checksum for each chunk is returned. The same is then done\non the follower and the checksums are compared, chunk by chunk. For\neach chunk, for which the checksums do not match, the list of keys and\nrevisions is fetched from the leader. This then enables the follower\nto fetch the actually needed documents and remove superfluous ones\nlocally.\n\nThis metric accumulates the time used for the second step of getting\nlists of key/revision pairs for each chunk.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 517
      },
      "id": 136,
      "targets": [
        {
          "expr": "rate(arangodb_replication_initial_keys_requests_time_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Accumulated wait time for replication keys requests",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This counter exhibits the accumulated wait time for replication key\nchunks determination requests, in milliseconds. This is part of the\nolder (pre 3.8) initial replication protocol, which might still be used\nin 3.8 for collections which have been created by older versions.\n\nIn this older protocol, the follower first fetches an overview over\na shard from the leader. This does a full collection scan and\ndivides the primary keys in the collection into equal sized chunks.\nThen, a checksum for each chunk is returned. The same is then done\non the follower and the checksums are compared, chunk by chunk. For\neach chunk, for which the checksums do not match, the list of keys and\nrevisions is fetched from the leader. This then enables the follower\nto fetch the actually needed documents and remove superfluous ones\nlocally.\n\nThis metric accumulates the time used for the initial step of getting\nthe checksums for the key chunks.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 525
      },
      "id": 133,
      "targets": [
        {
          "expr": "rate(arangodb_replication_initial_chunks_requests_time_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Accumulated wait time for replication key chunks determination requests",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This counter exhibits the accumulated number of bytes received\nfor initial synchronization of shards. This is part of the\nolder (pre 3.8) initial replication protocol, which might still be used\nin 3.8 for collections which have been created by older versions.\n\nIn this older protocol, the follower first fetches an overview over\na shard from the leader. This does a full collection scan and\ndivides the primary keys in the collection into equal sized chunks.\nThen, a checksum for each chunk is returned. The same is then done\non the follower and the checksums are compared, chunk by chunk. For\neach chunk, for which the checksums do not match, the list of keys and\nrevisions is fetched from the leader. This then enables the follower\nto fetch the actually needed documents and remove superfluous ones\nlocally.\n\nThis metric accumulates number of bytes received for all three steps.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 525
      },
      "id": 138,
      "targets": [
        {
          "expr": "rate(arangodb_replication_initial_sync_bytes_received_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Accumulated amount of bytes received in initial sync",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Accumulated time needed to apply replication initial sync insertions.\nThis counter exhibits the accumulated wait time for actually inserting\ndocuments for the initial synchronization, in milliseconds. This is\npart of the older (pre 3.8) initial replication protocol, which might\nstill be used in 3.8 for collections which have been created by older\nversions.\n\nIn this older protocol, the follower first fetches an overview over\na shard from the leader. This does a full collection scan and\ndivides the primary keys in the collection into equal sized chunks.\nThen, a checksum for each chunk is returned. The same is then done\non the follower and the checksums are compared, chunk by chunk. For\neach chunk, for which the checksums do not match, the list of keys and\nrevisions is fetched from the leader. This then enables the follower\nto fetch the actually needed documents and remove superfluous ones\nlocally.\n\nThis metric accumulates the time used for the actual insertion of\nreplicated documents on the follower.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 533
      },
      "id": 135,
      "targets": [
        {
          "expr": "rate(arangodb_replication_initial_insert_apply_time_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Accumulated time needed to apply replication initial sync insertions",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This counter exhibits the total number of documents removed on the\nfollower during initial synchronization of shards. This is part of the\nolder (pre 3.8) initial replication protocol, which might still be\nused in 3.8 for collections which have been created by older versions.\n\nIn this older protocol, the follower first fetches an overview over\na shard from the leader. This does a full collection scan and\ndivides the primary keys in the collection into equal sized chunks.\nThen, a checksum for each chunk is returned. The same is then done\non the follower and the checksums are compared, chunk by chunk. For\neach chunk, for which the checksums do not match, the list of keys and\nrevisions is fetched from the leader. This then enables the follower\nto fetch the actually needed documents and remove superfluous ones\nlocally.\n\nThis metric accumulates the total number of documents removed in the\nthird step.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 533
      },
      "id": 140,
      "targets": [
        {
          "expr": "rate(arangodb_replication_initial_sync_docs_removed_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of documents removed by replication initial sync",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This counter exhibits the accumulated wait time for removing local\ndocuments during initial synchronization of a shard on the follower,\nin milliseconds. This is part of the older (pre 3.8) initial\nreplication protocol, which might still be used in 3.8 for collections\nwhich have been created by older versions.\n\nIn this older protocol, the follower first fetches an overview over\na shard from the leader. This does a full collection scan and\ndivides the primary keys in the collection into equal sized chunks.\nThen, a checksum for each chunk is returned. The same is then done\non the follower and the checksums are compared, chunk by chunk. For\neach chunk, for which the checksums do not match, the list of keys and\nrevisions is fetched from the leader. This then enables the follower\nto fetch the actually needed documents and remove superfluous ones\nlocally.\n\nThis metric accumulates the time used for the intermediate step of\nremoving unneeded documents on the follower.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 541
      },
      "id": 137,
      "targets": [
        {
          "expr": "rate(arangodb_replication_initial_remove_apply_time_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Accumulated time needed to apply replication initial sync removals",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This counter exhibits the total number of times documents have been\nfetched on the follower from the leader during initial synchronization\nof shards. This is part of the older (pre 3.8) initial replication\nprotocol, which might still be used in 3.8 for collections which have\nbeen created by older versions.\n\nIn this older protocol, the follower first fetches an overview over\na shard from the leader. This does a full collection scan and\ndivides the primary keys in the collection into equal sized chunks.\nThen, a checksum for each chunk is returned. The same is then done\non the follower and the checksums are compared, chunk by chunk. For\neach chunk, for which the checksums do not match, the list of keys and\nrevisions is fetched from the leader. This then enables the follower\nto fetch the actually needed documents and remove superfluous ones\nlocally.\n\nThis metric accumulates the total number of times documents have been\nfetched from the leader in the third step.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 541
      },
      "id": 142,
      "targets": [
        {
          "expr": "rate(arangodb_replication_initial_sync_docs_requests_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of replication initial sync docs requests",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This counter exhibits the total number of documents inserted on the\nfollower during initial synchronization of shards. This is part of the\nolder (pre 3.8) initial replication protocol, which might still be\nused in 3.8 for collections which have been created by older versions.\n\nIn this older protocol, the follower first fetches an overview over\na shard from the leader. This does a full collection scan and\ndivides the primary keys in the collection into equal sized chunks.\nThen, a checksum for each chunk is returned. The same is then done\non the follower and the checksums are compared, chunk by chunk. For\neach chunk, for which the checksums do not match, the list of keys and\nrevisions is fetched from the leader. This then enables the follower\nto fetch the actually needed documents and remove superfluous ones\nlocally.\n\nThis metric accumulates the total number of documents inserted in the\nthird step.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 549
      },
      "id": 139,
      "targets": [
        {
          "expr": "rate(arangodb_replication_initial_sync_docs_inserted_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of documents inserted by replication initial sync",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "The total amount of all synchronous replication operation requests\nbetween DB-Servers being done.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 549
      },
      "id": 144,
      "targets": [
        {
          "expr": "rate(arangodb_replication_synchronous_requests_total_number_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total number of synchronous replication requests",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This counter exhibits the total number of documents fetched on the\nfollower from the leader during initial synchronization of shards.\nThis is part of the older (pre 3.8) initial replication protocol,\nwhich might still be used in 3.8 for collections which have been\ncreated by older versions.\n\nIn this older protocol, the follower first fetches an overview over\na shard from the leader. This does a full collection scan and\ndivides the primary keys in the collection into equal sized chunks.\nThen, a checksum for each chunk is returned. The same is then done\non the follower and the checksums are compared, chunk by chunk. For\neach chunk, for which the checksums do not match, the list of keys and\nrevisions is fetched from the leader. This then enables the follower\nto fetch the actually needed documents and remove superfluous ones\nlocally.\n\nThis metric accumulates the total number of documents fetched from the\nleader in the third step.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 557
      },
      "id": 141,
      "targets": [
        {
          "expr": "rate(arangodb_replication_initial_sync_docs_requested_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of documents requested by replication initial sync",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "The accumulated time needed to locally process the continuous\nreplication log on a follower received from a replication\nleader.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 557
      },
      "id": 146,
      "targets": [
        {
          "expr": "rate(arangodb_replication_tailing_apply_time_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Accumulated time needed to apply replication tailing data",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This counter exhibits the accumulated number of keys requests for\ninitial synchronization of shards. This is part of the\nolder (pre 3.8) initial replication protocol, which might still be used\nin 3.8 for collections which have been created by older versions.\n\nIn this older protocol, the follower first fetches an overview over\na shard from the leader. This does a full collection scan and\ndivides the primary keys in the collection into equal sized chunks.\nThen, a checksum for each chunk is returned. The same is then done\non the follower and the checksums are compared, chunk by chunk. For\neach chunk, for which the checksums do not match, the list of keys and\nrevisions is fetched from the leader. This then enables the follower\nto fetch the actually needed documents and remove superfluous ones\nlocally.\n\nThis metric counts the number of times the follower fetches a list of\nkeys for some chunk.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 565
      },
      "id": 143,
      "targets": [
        {
          "expr": "rate(arangodb_replication_initial_sync_keys_requests_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of replication initial sync keys requests",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "The accumulated number of replication tailing document inserts/replaces processed on a follower.",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 565
      },
      "id": 148,
      "targets": [
        {
          "expr": "rate(arangodb_replication_tailing_documents_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Accumulated number of replication tailing document inserts/replaces processed",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "The total time needed for all synchronous replication requests\nbetween DB-Servers being done.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 573
      },
      "id": 145,
      "targets": [
        {
          "expr": "rate(arangodb_replication_synchronous_requests_total_time_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total time needed for all synchronous replication requests",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "The number of replication tailing markers processed on a follower\nDB-Server. Markers are specific operations which are part of the\nwrite-ahead log (WAL). Example actions which are being used in\nmarkers: Create or drop a database. Create, drop, rename, change\nor truncate a collection. Create or drop an index. Create, drop,\nchange a view. Start, commit or abort a transaction.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 573
      },
      "id": 150,
      "targets": [
        {
          "expr": "rate(arangodb_replication_tailing_markers_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of replication tailing markers processed",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "The accumulated number of bytes received from a leader for replication tailing requests. The higher the amount of bytes is, the more data is being processed afterwards on the follower DB-Server.",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 581
      },
      "id": 147,
      "targets": [
        {
          "expr": "rate(arangodb_replication_tailing_bytes_received_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Accumulated number of bytes received for replication tailing requests",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Aggregated wait time for replication tailing requests.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 581
      },
      "id": 152,
      "targets": [
        {
          "expr": "rate(arangodb_replication_tailing_request_time_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Aggregated wait time for replication tailing requests",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "The number of replication tailing failures due to missing tick on leader.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 589
      },
      "id": 149,
      "targets": [
        {
          "expr": "rate(arangodb_replication_tailing_follow_tick_failures_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of replication tailing failures due to missing tick on leader",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "The revision trees of collections/shards are normally present\nin RAM in an uncompressed state. However, to reduce the memory\nusage of keeping all revision trees in RAM at the same time, \nrevision trees can be put into \"hibernation\" mode. Any inactive\nrevision tree will automatically be hibernated by ArangoDB after\na while. For the hibernation step, a revision tree will be \ncompressed in RAM, and only the compressed version is then kept.\nLater accesses of a compressed revision tree require uncompressing\nthe tree again. \nThis metric is increased whenever a revision tree is hibernated.\nThis can happened many times during the lifetime of a revision tree.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 589
      },
      "id": 154,
      "targets": [
        {
          "expr": "rate(arangodb_revision_tree_hibernations_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of revision tree hibernations",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "The amount of document removal based marker operations on a\nfollower DB-Server.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 597
      },
      "id": 151,
      "targets": [
        {
          "expr": "rate(arangodb_replication_tailing_removals_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of replication tailing document removals processed",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Number of failed background revision tree rebuilds.\nIdeally this value stays at 0, because if a revision tree rebuild\nfails, the system may stall and not be able to make progress in\nterms of WAL file collection. In case the counter is increased,\nan error message will also be logged to the arangod logfile.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 597
      },
      "id": 156,
      "targets": [
        {
          "expr": "rate(arangodb_revision_tree_rebuilds_failure_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of failed revision tree rebuilds",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "The total amount of network replication tailing requests.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 605
      },
      "id": 153,
      "targets": [
        {
          "expr": "rate(arangodb_replication_tailing_requests_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of replication tailing requests",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "The revision trees of collections/shards are normally present\nin RAM in an uncompressed state. However, to reduce the memory\nusage of keeping all revision trees in RAM at the same time, \nrevision trees can be put into \"hibernation\" mode. Any inactive\nrevision tree will automatically be hibernated by ArangoDB after\na while. For the hibernation step, a revision tree will be \ncompressed in RAM, and only the compressed version is then kept.\nLater accesses of a compressed revision tree require uncompressing\nthe tree again. \nThis metric is increased whenever a revision tree is restored from\nits hibernated state back into an uncompressed form in RAM.\nThis can happened many times during the lifetime of a revision tree.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 605
      },
      "id": 158,
      "targets": [
        {
          "expr": "rate(arangodb_revision_tree_resurrections_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of revision tree resurrections",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total memory usage of all revision trees for collections/shards.\nThe revision trees of collections/shards are normally present\nin RAM in an uncompressed state. However, to reduce the memory\nusage of keeping all revision trees in RAM at the same time, \nrevision trees can be put into \"hibernation\" mode. Any inactive\nrevision tree will automatically be hibernated by ArangoDB after\na while. For the hibernation step, a revision tree will be \ncompressed in RAM, and only the compressed version is then kept.\nLater accesses of a compressed revision tree require uncompressing\nthe tree again. \nThis metrics reports the total memory usage of all revision trees,\nincluding both the hibernated and uncompressed forms).\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 613
      },
      "id": 155,
      "targets": [
        {
          "expr": "arangodb_revision_tree_memory_usage",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total memory usage of all revision trees (both hibernated and uncompressed)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Number of shards not replicated at all. This is counted for all shards\nfor which this server is currently the leader. The number is increased\nby one for every shards for which no follower is in sync.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 613
      },
      "id": 160,
      "targets": [
        {
          "expr": "arangodb_shards_not_replicated",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of shards not replicated at all",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Number of successful background revision tree rebuilds.\nIdeally this value stays at 0, because a revision tree rebuild\nindicates a problem with a collection/shard's revision tree that\nhas happened before.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 621
      },
      "id": 157,
      "targets": [
        {
          "expr": "rate(arangodb_revision_tree_rebuilds_success_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of successful revision tree rebuilds",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Number of leader shards not fully replicated. This is counted for all\nshards for which this server is currently the leader. The number is\nincreased by one for every shards for which not all followers are in sync.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 621
      },
      "id": 162,
      "targets": [
        {
          "expr": "arangodb_shards_out_of_sync",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of leader shards not fully replicated",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Number of leader shards on this machine. Every shard has a leader and\npotentially multiple followers.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 629
      },
      "id": 159,
      "targets": [
        {
          "expr": "arangodb_shards_leader_number",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of leader shards on this machine",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Number of times a mismatching shard checksum was detected when\nsyncing shards. This is a very special metric which is rarely used.\nWhen followers of shards get in sync with their leaders, just when\neverything is completed a final checksum is taken as an additional\nprecaution. If this checksum differs between leader an follower, the\nincremental resync process starts from scratch.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 629
      },
      "id": 164,
      "targets": [
        {
          "expr": "rate(arangodb_sync_wrong_checksum_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of times a mismatching shard checksum was detected when syncing shards",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Number of shards on this machine. Every shard has a leader and\npotentially multiple followers. This metric counts both leader and\nfollower shards.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 637
      },
      "id": 161,
      "targets": [
        {
          "expr": "arangodb_shards_number",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of shards on this machine",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Number of times a follower shard needed to be completely rebuilt\nbecause of too many subsequent shard synchronization failures.\nIf this metric is non-zero, it means that a follower shard could\nnot get in sync with the leader even after many attempts. When\nthe metric gets increased, the follower shard is dropped and\ncompletely rebuilt from leader data, in order to increase its\nchances of getting in sync.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 645
      },
      "id": 163,
      "targets": [
        {
          "expr": "rate(arangodb_sync_rebuilds_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of times a follower shard needed to be completely rebuilt because of\ntoo many synchronization failures",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 653
      },
      "id": 165,
      "panels": [],
      "title": "Errors",
      "type": "row"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total number of errors (ERR messages) logged by the logger. \n\nIf a problem is encountered which is fatal to some operation, but not for\nthe service or the application as a whole, then an _error is logged.\n\nReasons for log entries of this severity are for example include missing\ndata, inability to open required files, incorrect connection strings,\nmissing services.\n\nIf an error is logged then it should be taken seriously as it may require \nuser intervention to solve.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 654
      },
      "id": 166,
      "targets": [
        {
          "expr": "rate(arangodb_logger_errors_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total number of errors logged",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total number of warnings (WARN messages) logged by the logger, \nincluding startup warnings.\n\nWarnings might indicate problems, or might not. For example, \nexpected transient environmental conditions such as short loss of \nnetwork or database connectivity are logged as warnings, not errors. \n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 654
      },
      "id": 167,
      "targets": [
        {
          "expr": "rate(arangodb_logger_warnings_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total number of warnings logged",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 662
      },
      "id": 168,
      "panels": [],
      "title": "RocksDB",
      "type": "row"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of the collection/shard lock acquisition times. Locks will be acquired for\nall write operations, but not for read operations.\nThe values here are measured in seconds.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 663
      },
      "id": 169,
      "legend": {
        "show": false
      },
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(arangodb_collection_lock_acquisition_time_bucket[3m])) by (le))",
          "format": "heatmap",
          "legendFormat": "",
          "refId": "A"
        }
      ],
      "title": "Collection lock acquisition time histogram",
      "type": "heatmap"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of the collection/shard lock acquisition times. Locks will be acquired for\nall write operations, but not for read operations.\nThe values here are measured in seconds.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 663
      },
      "id": 170,
      "targets": [
        {
          "expr": "rate(arangodb_collection_lock_acquisition_time_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Collection lock acquisition time histogram (count of events per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of the collection/shard lock acquisition times. Locks will be acquired for\nall write operations, but not for read operations.\nThe values here are measured in seconds.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 671
      },
      "id": 171,
      "targets": [
        {
          "expr": "rate(arangodb_collection_lock_acquisition_time_sum[3m]) / rate(arangodb_collection_lock_acquisition_time_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Collection lock acquisition time histogram (average per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This counter reflects the number of times RocksDB was observed by\nArangoDB to have entered a stalled (slowed) write state.\n\nIf the RocksDB background threads which do cleanup and compaction\ncannot keep up with the writing, then RocksDB first throttles its\nwrite rate (\"write stall\") and later stops the writing entirely\n(\"write stop\"). Both are suboptimal, since the write rate is too high.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 671
      },
      "id": 172,
      "targets": [
        {
          "expr": "rate(arangodb_rocksdb_write_stalls_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of times RocksDB has entered a stalled (slowed) write state",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This counter reflects the number of times RocksDB was observed by\nArangoDB to have entered a stopped write state.\n\nIf the RocksDB background threads which do cleanup and compaction\ncannot keep up with the writing, then RocksDB first throttles its\nwrite rate (\"write stall\") and later stops the writing entirely\n(\"write stop\"). Both are suboptimal, since the write rate is too high,\nbut write stops are considerably worse, since they can lead to service\nunavailability.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 679
      },
      "id": 173,
      "targets": [
        {
          "expr": "rate(arangodb_rocksdb_write_stops_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of times RocksDB has entered a stopped write state",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the RocksDB metric \"rocksdb-actual-delayed-write-rate\".\nIt shows the current actual delayed write rate. The value 0 means no delay.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 679
      },
      "id": 174,
      "targets": [
        {
          "expr": "rocksdb_actual_delayed_write_rate",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Actual delayed RocksDB write rate",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the total number of RocksDB WAL files in the\n\"archive\" subdirectory. These are WAL files that can be garbage-collected\neventually, when they are not used anymore by replication or other WAL\ntailing.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 687
      },
      "id": 175,
      "targets": [
        {
          "expr": "rocksdb_archived_wal_files",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of RocksDB WAL files in the archive",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the RocksDB metric \"background-errors\". It shows\nthe accumulated number of background errors.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 687
      },
      "id": 176,
      "targets": [
        {
          "expr": "rocksdb_background_errors",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total number of RocksDB background errors",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the RocksDB metric \"rocksdb-base-level\".\nIt shows the number of the level to which L0 data will be\ncompacted.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 695
      },
      "id": 177,
      "targets": [
        {
          "expr": "rocksdb_base_level",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "RocksDB base level",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the RocksDB metric \"rocksdb-block-cache-capacity\".\nIt shows the block cache capacity in bytes. This can be configured with\nthe `--rocksdb.block-cache-size` startup option.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 695
      },
      "id": 178,
      "targets": [
        {
          "expr": "rocksdb_block_cache_capacity",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "RocksDB block cache capacity",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the RocksDB metric \"rocksdb-block-cache-pinned-usage\".\nIt shows the memory size for the RocksDB block cache for the entries\nwhich are pinned, in bytes.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 703
      },
      "id": 179,
      "targets": [
        {
          "expr": "rocksdb_block_cache_pinned_usage",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Size of pinned RocksDB block cache entries",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the RocksDB metric \"rocksdb-block-cache-usage\".\nIt shows the total memory size for the entries residing in the block cache,\nin bytes.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 703
      },
      "id": 180,
      "targets": [
        {
          "expr": "rocksdb_block_cache_usage",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Cumulated size of RocksDB block cache entries",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric reflects the current global allocation for the ArangoDB\ncache which sits in front of RocksDB. For example, the edge cache\ncounts towards this allocation. All these caches together have a\nglobal limit which can be controlled with the `--cache.size` startup option.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 711
      },
      "id": 181,
      "targets": [
        {
          "expr": "rocksdb_cache_allocated",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Global current allocation of ArangoDB cache",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric reflects the lifetime hit rate of the ArangoDB in-memory\ncache which is sitting in front of RocksDB. For example, the edge\ncache is a part of this. The value will be a ratio between 0 and 1.\n\"Lifetime\" means here that accounting is done from the most recent\nstart of the `arangod` instance.\nIf the hit rate is too low, you might have to little RAM available\nfor the in-memory caches.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 711
      },
      "id": 182,
      "targets": [
        {
          "expr": "rocksdb_cache_hit_rate_lifetime",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Lifetime hit rate of the ArangoDB cache in front of RocksDB",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric reflects the recent hit rate of the ArangoDB in-memory\ncache which is sitting in front of RocksDB. For example, the edge\ncache is a part of this. The value will be a ratio between 0 and 1.\nIf the hit rate is too low, you might have to little RAM available\nfor the in-memory caches.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 719
      },
      "id": 183,
      "targets": [
        {
          "expr": "rocksdb_cache_hit_rate_recent",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Recent hit rate of the ArangoDB cache in front of RocksDB",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric reflects the current global allocation limit for the\nArangoDB caches which sit in front of RocksDB. For example, the\nedge cache counts towards this allocation. This global limit can\nbe controlled with the `--cache.size` startup option.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 719
      },
      "id": 184,
      "targets": [
        {
          "expr": "rocksdb_cache_limit",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Global allocation limit for the ArangoDB cache in front of RocksDB",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the RocksDB metric \"compaction-pending\".\nIt shows the number of column families for which at least one compaction\nis pending.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 727
      },
      "id": 185,
      "targets": [
        {
          "expr": "rocksdb_compaction_pending",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of RocksDB column families with pending compaction",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the compression ratio of data at level 0 in RocksDB's\nlog structured merge tree. Here, compression\nratio is defined as uncompressed data size / compressed file size.\nReturns \"-1.0\" if there are no open files at level 0.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 727
      },
      "id": 186,
      "targets": [
        {
          "expr": "rocksdb_compression_ratio_at_level0",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "RocksDB compression ratio at level 0",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the compression ratio of data at level 1 in RocksDB's\nlog structured merge tree. Here, compression\nratio is defined as uncompressed data size / compressed file size.\nReturns \"-1.0\" if there are no open files at level 1.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 735
      },
      "id": 187,
      "targets": [
        {
          "expr": "rocksdb_compression_ratio_at_level1",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "RocksDB compression ratio at level 1",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the compression ratio of data at level 2 in RocksDB's\nlog structured merge tree. Here, compression\nratio is defined as uncompressed data size / compressed file size.\nReturns \"-1.0\" if there are no open files at level 2.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 735
      },
      "id": 188,
      "targets": [
        {
          "expr": "rocksdb_compression_ratio_at_level2",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "RocksDB compression ratio at level 2",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the compression ratio of data at level 3 in RocksDB's\nlog structured merge tree. Here, compression\nratio is defined as uncompressed data size / compressed file size.\nReturns \"-1.0\" if there are no open files at level 3.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 743
      },
      "id": 189,
      "targets": [
        {
          "expr": "rocksdb_compression_ratio_at_level3",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "RocksDB compression ratio at level 3",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the compression ratio of data at level 4 in RocksDB's\nlog structured merge tree. Here, compression\nratio is defined as uncompressed data size / compressed file size.\nReturns \"-1.0\" if there are no open files at level 4.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 743
      },
      "id": 190,
      "targets": [
        {
          "expr": "rocksdb_compression_ratio_at_level4",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "RocksDB compression ratio at level 4",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the compression ratio of data at level 5 in RocksDB's\nlog structured merge tree. Here, compression\nratio is defined as uncompressed data size / compressed file size.\nReturns \"-1.0\" if there are no open files at level 5.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 751
      },
      "id": 191,
      "targets": [
        {
          "expr": "rocksdb_compression_ratio_at_level5",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "RocksDB compression ratio at level 5",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the compression ratio of data at level 6 in RocksDB's\nlog structured merge tree. Here, compression\nratio is defined as uncompressed data size / compressed file size.\nReturns \"-1.0\" if there are no open files at level 6.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 751
      },
      "id": 192,
      "targets": [
        {
          "expr": "rocksdb_compression_ratio_at_level6",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "RocksDB compression ratio at level 6",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the RocksDB metric \"rocksdb-cur-size-active-mem-table\".\nIt shows the approximate size of the active memtable in bytes, summed\nover all column families.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 759
      },
      "id": 193,
      "targets": [
        {
          "expr": "rocksdb_cur_size_active_mem_table",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Approximate size of RocksDB's active memtable",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the RocksDB metric \"rocksdb-cur-size-all-mem-tables\".\nIt shows the approximate size of active and unflushed immutable memtables\nin bytes, summed over all column families.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 759
      },
      "id": 194,
      "targets": [
        {
          "expr": "rocksdb_cur_size_all_mem_tables",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Approximate size of all active and unflushed RocksDB memtables",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exposes the current write rate limit of the ArangoDB\nRocksDB throttle. The throttle limits the write rate to allow\nRocksDB's background threads to catch up with compactions and not\nfall behind too much, since this would in the end lead to nasty\nwrite stops in RocksDB and incur considerable delays. If 0 is\nshown, no throttling happens, otherwise, you see the current\nwrite rate limit in bytes per second. Also see the `--rocksdb.*`\nstartup options.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 767
      },
      "id": 195,
      "targets": [
        {
          "expr": "rocksdb_engine_throttle_bps",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Current rate of the RocksDB throttle in bytes per second",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the RocksDB metric \"rocksdb-estimate-live-data-size\".\nIt shows an estimate of the amount of live data in bytes, summed over\nall column families.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 767
      },
      "id": 196,
      "targets": [
        {
          "expr": "rocksdb_estimate_live_data_size",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Estimated amount of live RocksDB data",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the RocksDB metric \"rocksdb-estimate-num-keys\".\nIt shows the estimated number of total keys in the active and unflushed\nimmutable memtables and storage, summed over all column families.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 775
      },
      "id": 197,
      "targets": [
        {
          "expr": "rocksdb_estimate_num_keys",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Estimated number of RocksDB keys",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the RocksDB metric\n\"rocksdb-estimate-pending-compaction-bytes\".\nIt shows the estimated total number of bytes compaction needs to\nrewrite to get all levels down to under target size. Not valid for\nother compactions than level-based. This value is summed over all\ncolumn families.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 775
      },
      "id": 198,
      "targets": [
        {
          "expr": "rocksdb_estimate_pending_compaction_bytes",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Estimated number of bytes awaiting RocksDB compaction",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the RocksDB metric\n\"rocksdb-estimate-table-readers-mem\".\nIt shows the estimated memory used for reading SST tables, excluding\nmemory used in block cache (e.g. filter and index blocks), summed over\nall column families.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 783
      },
      "id": 199,
      "targets": [
        {
          "expr": "rocksdb_estimate_table_readers_mem",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Estimated memory usage for reading RocksDB SST tables",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric shows the currently free disk space in bytes on the volume\nwhich is used by RocksDB. Since RocksDB does not like out of disk\nspace scenarios, please make sure that there is enough free disk space\navailable at all times!  Note that this metric is only available/populated on some platforms.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 783
      },
      "id": 200,
      "targets": [
        {
          "expr": "rocksdb_free_disk_space",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Free disk space in bytes on volume used by RocksDB",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric shows the currently free number of inodes on the disk volume\nused by RocksDB. Since RocksDB does not like out of disk space\nscenarios, please make sure that there is enough free inodes available\nat all times! Note that this metric is only available/populated on some platforms.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 791
      },
      "id": 201,
      "targets": [
        {
          "expr": "rocksdb_free_inodes",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of free inodes on the volume used by RocksDB",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the RocksDB metric \"rocksdb-is-file-deletions-enabled\".\nIt shows 0 if deletion of obsolete files is enabled, and otherwise,\nit shows a non-zero number. Note that for ArangoDB, this is supposed\nto always return 1, since the deletion of obsolete WAL files is done\nfrom ArangoDB, externally to RocksDB.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 791
      },
      "id": 202,
      "targets": [
        {
          "expr": "rocksdb_is_file_deletions_enabled",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Whether RocksDB file deletion is enabled",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the RocksDB metric \"rocksdb-is-write-stopped\".\nIt shows 1 if writing to RocksDB has been stopped, and otherwise 0.\nIf 1 is shown, this usually means that there are too many uncompacted\nfiles and the RocksDB background threads have not managed to keep up\nwith their compaction work. This situation should be avoided, since\nnasty delays in database operations are incurred. If in doubt,\ncontact ArangoDB support.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 799
      },
      "id": 203,
      "targets": [
        {
          "expr": "rocksdb_is_write_stopped",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Whether RocksDB writes are stopped",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the RocksDB metric \"rocksdb-live-sst-files-size\".\nIt shows the total size in bytes of all SST files belonging to the latest\nLSM tree, summed over all column families.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 799
      },
      "id": 204,
      "targets": [
        {
          "expr": "rocksdb_live_sst_files_size",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Size of live RocksDB SST files",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the RocksDB metric \"mem-table-flush-pending\". It\nshows the number of column families for which a memtable flush is\npending.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 807
      },
      "id": 205,
      "targets": [
        {
          "expr": "rocksdb_mem_table_flush_pending",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of RocksDB column families awaiting memtable flush",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the RocksDB metric \"rocksdb-min-log-number-to-keep\".\nIt shows the minimum log number of the log files that should be kept.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 807
      },
      "id": 206,
      "targets": [
        {
          "expr": "rocksdb_min_log_number_to_keep",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Minimum number of RocksDB log files to keep",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the RocksDB metric\n\"rocksdb-num-deletes-active-mem-table\".\nIt shows the total number of delete entries in the active memtable,\nsummed over all column families.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 815
      },
      "id": 207,
      "targets": [
        {
          "expr": "rocksdb_num_deletes_active_mem_table",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of deletes in active RocksDB memtable",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the RocksDB metric\n\"rocksdb-num-deletes-imm-mem-tables\".\nIt shows the total number of delete entries in the unflushed immutable\nmemtables, summed over all column families.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 815
      },
      "id": 208,
      "targets": [
        {
          "expr": "rocksdb_num_deletes_imm_mem_tables",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of deletes in unflushed immutable RocksDB memtables",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the RocksDB metric\n\"rocksdb-num-entries-active-mem-table\".\nIt shows the total number of entries in the active memtable,\nsummed over all column families.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 823
      },
      "id": 209,
      "targets": [
        {
          "expr": "rocksdb_num_entries_active_mem_table",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of entries in the active RocksDB memtable",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the RocksDB metric\n\"rocksdb-num-entries-imm-mem-tables\".\nIt shows the total number of entries in the unflushed immutable memtables,\nsummed over all column families.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 823
      },
      "id": 210,
      "targets": [
        {
          "expr": "rocksdb_num_entries_imm_mem_tables",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of entries in unflushed immutable RocksDB memtables",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric reports the number of files at level 0 in the log structured\nmerge tree of RocksDB.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 831
      },
      "id": 211,
      "targets": [
        {
          "expr": "rocksdb_num_files_at_level0",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of RocksDB files at level 0",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric reports the number of files at level 1 in the log structured\nmerge tree of RocksDB.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 831
      },
      "id": 212,
      "targets": [
        {
          "expr": "rocksdb_num_files_at_level1",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of RocksDB files at level 1",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric reports the number of files at level 2 in the log structured\nmerge tree of RocksDB.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 839
      },
      "id": 213,
      "targets": [
        {
          "expr": "rocksdb_num_files_at_level2",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of RocksDB files at level 2",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric reports the number of files at level 3 in the log structured\nmerge tree of RocksDB.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 839
      },
      "id": 214,
      "targets": [
        {
          "expr": "rocksdb_num_files_at_level3",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of RocksDB files at level 3",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric reports the number of files at level 4 in the log structured\nmerge tree of RocksDB.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 847
      },
      "id": 215,
      "targets": [
        {
          "expr": "rocksdb_num_files_at_level4",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of RocksDB files at level 4",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric reports the number of files at level 5 in the log structured\nmerge tree of RocksDB.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 847
      },
      "id": 216,
      "targets": [
        {
          "expr": "rocksdb_num_files_at_level5",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of RocksDB files at level 5",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric reports the number of files at level 6 in the log structured\nmerge tree of RocksDB.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 855
      },
      "id": 217,
      "targets": [
        {
          "expr": "rocksdb_num_files_at_level6",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of RocksDB files at level 6",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the RocksDB metric \"num-immutable-mem-table\",\nwhich shows the number of immutable memtables that have not yet been\nflushed. This value is the sum over all column families.\n\nMemtables are sorted tables of key/value pairs which begin\nto be built up in memory. At some stage they are closed and become\nimmutable, and some time later they are flushed to disk.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 855
      },
      "id": 218,
      "targets": [
        {
          "expr": "rocksdb_num_immutable_mem_table",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of unflushed immutable RocksDB memtables",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the RocksDB metric \"num-immutable-mem-table-flushed\",\nwhich shows the number of immutable memtables that have already been\nflushed. This value is the sum over all column families.\n\nMemtables are sorted tables of key/value pairs which begin\nto be built up in memory. At some stage they are closed and become\nimmutable, and some time later they are flushed to disk.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 863
      },
      "id": 219,
      "targets": [
        {
          "expr": "rocksdb_num_immutable_mem_table_flushed",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of flushed immutable RocksDB memtables",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the RocksDB metric \"rocksdb-num-live-versions\".\nIt shows the number of live versions. `Version` is an internal data\nstructure. See `version_set.h` in the RocksDB source for details. More\nlive versions often mean more SST files are held from being deleted,\nby iterators or unfinished compactions. This number is the number\nsummed up over all column families.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 863
      },
      "id": 220,
      "targets": [
        {
          "expr": "rocksdb_num_live_versions",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of live RocksDB versions",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the RocksDB metric \"rocksdb-num-running-compactions\".\nIt shows the number of currently running compactions.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 871
      },
      "id": 221,
      "targets": [
        {
          "expr": "rocksdb_num_running_compactions",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of running RocksDB compactions",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the RocksDB metric \"rocksdb-num-running-flushes\".\nIt shows the number of currently running flushes.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 871
      },
      "id": 222,
      "targets": [
        {
          "expr": "rocksdb_num_running_flushes",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of running RocksDB flushes",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the RocksDB metric \"rocksdb-num-snapshots\".\nIt shows the number of unreleased snapshots of the database.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 879
      },
      "id": 223,
      "targets": [
        {
          "expr": "rocksdb_num_snapshots",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of unreleased RocksDB snapshots",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the RocksDB metric \"rocksdb-oldest-snapshot-time\".\nIt shows a number representing the Unix timestamp of the oldest\nunreleased snapshot.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 879
      },
      "id": 224,
      "targets": [
        {
          "expr": "rocksdb_oldest_snapshot_time",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Timestamp of oldest unreleased RocksDB snapshot",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the total number of RocksDB WAL files in the\n\"archive\" subdirectory that can be pruned. These are WAL files that\ncan be pruned by a background thread to reclaim disk space.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 887
      },
      "id": 225,
      "targets": [
        {
          "expr": "rocksdb_prunable_wal_files",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of prunable RocksDB WAL files in the archive",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric indicates whether RocksDB currently is in read-only\nmode, due to a background error. If RocksDB is in read-only mode,\nthis metric will have a value of \"1\". When in read-only mode, all\nwrites into RocksDB will fail. When RocksDB is in normal operations\nmode, this metric will have a value of \"0\".\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 887
      },
      "id": 226,
      "targets": [
        {
          "expr": "rocksdb_read_only",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "RocksDB metric \"background-errors\"",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the RocksDB metric \"rocksdb-size-all-mem-tables\".\nIt shows the approximate size of all active, unflushed immutable, and\npinned immutable memtables in bytes, summed over all column families.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 895
      },
      "id": 227,
      "targets": [
        {
          "expr": "rocksdb_size_all_mem_tables",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Approximate size of all RocksDB memtables",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric shows the currently used disk space in bytes on the volume\nwhich is used by RocksDB. Since RocksDB does not like out of disk\nspace scenarios, please make sure that there is enough free disk space\navailable at all times! Note that this metric is only available/populated on some platforms.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 895
      },
      "id": 228,
      "targets": [
        {
          "expr": "rocksdb_total_disk_space",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Used disk space in bytes on volume used by RocksDB",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric shows the currently used number of inodes on the disk volume\nused by RocksDB. Since RocksDB does not like out of disk space\nscenarios, please make sure that there are enough free inodes available\nat all times! Note that this metric is only available/populated on some platforms.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 903
      },
      "id": 229,
      "targets": [
        {
          "expr": "rocksdb_total_inodes",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of used inodes on the volume used by RocksDB",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exhibits the RocksDB metric \"rocksdb-total-sst-files-size\".\nIt shows the total size in bytes of all SST files, summed over all\ncolumn families.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 903
      },
      "id": 230,
      "targets": [
        {
          "expr": "rocksdb_total_sst_files_size",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Size of all RocksDB SST files",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric contains a value of 0 if the pruning of archived RocksDB WAL\nfiles is not activated, and 1 if it is activated.\nWAL file pruning is normally deactivated for the first few minutes after\nan instance is started, so that other instances in the cluster can start\nreplicating from the instance before all archived WAL files are deleted.\nThe value should flip from 0 to 1 a few minutes after server start.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 911
      },
      "id": 231,
      "targets": [
        {
          "expr": "rocksdb_wal_pruning_active",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Whether or not the pruning of archived RocksDB WAL files is currently\nactivated",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exposes the current RocksDB WAL sequence number. Any\nwrite operations into the database will increase the sequence number.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 911
      },
      "id": 232,
      "targets": [
        {
          "expr": "rocksdb_wal_sequence",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Current RocksDB WAL sequence number",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric exposes the RocksDB WAL sequence number until which the\nArangoDB background sync thread has fully caught up to. The value exposed\nhere should be monotically increasing and always progress if there are\nwrite operations executing.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 919
      },
      "id": 233,
      "targets": [
        {
          "expr": "rocksdb_wal_sequence_lower_bound",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "RocksDB sequence number until which the background sync thread\nhas caught up",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Current number of connections in pool. There are two pools, one for the\nAgency communication with label `AgencyComm` and one for the other\ncluster internal communication with label `ClusterComm`.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 919
      },
      "id": 236,
      "targets": [
        {
          "expr": "arangodb_connection_pool_connections_current",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Current number of connections in pool",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 927
      },
      "id": 234,
      "panels": [],
      "title": "Connectivity",
      "type": "row"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total number of connections created for connection pool. There are\ntwo pools, one for the Agency communication with label `AgencyComm`\nand one for the other cluster internal communication with label\n`ClusterComm`.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 928
      },
      "id": 235,
      "targets": [
        {
          "expr": "rate(arangodb_connection_pool_connections_created_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total number of connections created for connection pool",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Time to lease a connection from the connection pool. There are two pools,\none for the Agency communication with label `AgencyComm` and one for\nthe other cluster internal communication with label `ClusterComm`.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 928
      },
      "id": 238,
      "targets": [
        {
          "expr": "rate(arangodb_connection_pool_lease_time_hist_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Time to lease a connection from the connection pool (count of events per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Time to lease a connection from the connection pool. There are two pools,\none for the Agency communication with label `AgencyComm` and one for\nthe other cluster internal communication with label `ClusterComm`.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 936
      },
      "id": 237,
      "legend": {
        "show": false
      },
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(arangodb_connection_pool_lease_time_hist_bucket[3m])) by (le))",
          "format": "heatmap",
          "legendFormat": "",
          "refId": "A"
        }
      ],
      "title": "Time to lease a connection from the connection pool",
      "type": "heatmap"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total number of failed connection leases. There are two pools, one for\nthe Agency communication with label `AgencyComm` and one for the other\ncluster internal communication with label `ClusterComm`.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 936
      },
      "id": 240,
      "targets": [
        {
          "expr": "rate(arangodb_connection_pool_leases_failed_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total number of failed connection leases",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Time to lease a connection from the connection pool. There are two pools,\none for the Agency communication with label `AgencyComm` and one for\nthe other cluster internal communication with label `ClusterComm`.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 944
      },
      "id": 239,
      "targets": [
        {
          "expr": "rate(arangodb_connection_pool_lease_time_hist_sum[3m]) / rate(arangodb_connection_pool_lease_time_hist_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Time to lease a connection from the connection pool (average per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total number of connections accepted for HTTP/2, this can be upgraded\nconnections from HTTP/1.1 or connections negotiated to be HTTP/2 during\nthe TLS handshake.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 944
      },
      "id": 242,
      "targets": [
        {
          "expr": "rate(arangodb_http2_connections_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total number of HTTP/2 connections accepted",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total number of successful connection leases from connection pool.\nThere are two pools, one for the Agency communication with label\n`AgencyComm` and one for the other cluster internal communication with\nlabel `ClusterComm`.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 952
      },
      "id": 241,
      "targets": [
        {
          "expr": "rate(arangodb_connection_pool_leases_successful_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total number of successful connection leases from connection pool",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This histogram shows how long requests to the Agency took.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 952
      },
      "id": 246,
      "targets": [
        {
          "expr": "rate(arangodb_agencycomm_request_time_msec_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Request time for Agency requests (count of events per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total number of connections accepted for VST, this are upgraded\nconnections from HTTP/1.1.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 960
      },
      "id": 243,
      "targets": [
        {
          "expr": "rate(arangodb_vst_connections_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total number of VST connections accepted",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Number of requests forwarded to another Coordinator.\nRequest forwarding can happen in load-balanced setups,\nwhen one Coordinator receives and forwards requests\nthat can only be handled by a different Coordinator.\nThis includes requests for streaming transactions,\nAQL, query cursors, Pregel jobs and some others.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 960
      },
      "id": 248,
      "targets": [
        {
          "expr": "rate(arangodb_network_forwarded_requests_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of requests forwarded to another Coordinator",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 968
      },
      "id": 244,
      "panels": [],
      "title": "Network",
      "type": "row"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This histogram shows how long requests to the Agency took.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 969
      },
      "id": 245,
      "legend": {
        "show": false
      },
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(arangodb_agencycomm_request_time_msec_bucket[3m])) by (le))",
          "format": "heatmap",
          "legendFormat": "",
          "refId": "A"
        }
      ],
      "title": "Request time for Agency requests",
      "type": "heatmap"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Number of internal requests that have timed out. This metric is increased\nwhenever any cluster-internal request executed in the underlying connection\npool runs into a timeout.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 969
      },
      "id": 250,
      "targets": [
        {
          "expr": "rate(arangodb_network_request_timeouts_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of internal requests that have timed out",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This histogram shows how long requests to the Agency took.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 977
      },
      "id": 247,
      "targets": [
        {
          "expr": "rate(arangodb_agencycomm_request_time_msec_sum[3m]) / rate(arangodb_agencycomm_request_time_msec_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Request time for Agency requests (average per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This counter reflects the accumulated total time for creating V8\ncontexts, in milliseconds. It is OK if this number keeps growing since\nthe V8 contexts are created and destroyed as needed. In rare cases a\nhigh fluctuation can indicate some unfortunate usage pattern.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 977
      },
      "id": 254,
      "targets": [
        {
          "expr": "rate(arangodb_v8_context_creation_time_msec_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Accumulated total time for creating V8 contexts",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram providing the round-trip time of internal requests as a percentage\nof the respective request timeout.\nThis metric will provide values between 0 and 100.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 985
      },
      "id": 249,
      "targets": [
        {
          "expr": "arangodb_network_request_duration_as_percentage_of_timeout",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Internal request round-trip time as a percentage of timeout",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total number of V8 context enter failures. A context receives a context\nenter event every time it begins to execute some JavaScript. If no\ncontext is available at such a time the system waits for 60s for a\ncontext to become free. If this does not happen within the 60s, the\ncontext enter event fails, a warning is logged and this counter is\nincreased by one.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 985
      },
      "id": 256,
      "targets": [
        {
          "expr": "rate(arangodb_v8_context_enter_failures_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total number of V8 context enter failures",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Number of outgoing internal requests in flight. This metric is increased\nwhenever any cluster-internal request is about to be sent via the underlying\nconnection pool, and is decreased whenever a response for such a request is\nreceived or the request runs into a timeout.\nThis metric provides an estimate of the fan-out of operations. For example,\na user operation on a collection with a single shard will normally lead to\na single internal request (plus replication), whereas an operation on a\ncollection with 10 shards may lead to a fan-out of 10 (plus replication).\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 993
      },
      "id": 251,
      "targets": [
        {
          "expr": "arangodb_network_requests_in_flight",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of outgoing internal requests in flight",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This counter reflects the total number of V8 context exit events.\nA context receives a context exit event every time it finishes to\nexecute some JavaScript.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 993
      },
      "id": 258,
      "targets": [
        {
          "expr": "rate(arangodb_v8_context_exited_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total number of V8 context exit events",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 1001
      },
      "id": 252,
      "panels": [],
      "title": "V8",
      "type": "row"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This counter reflects the total number of V8 contexts ever created. It is\nOK if this number keeps growing since the V8 contexts are created and\ndestroyed as needed. In rare cases a high fluctuation can indicate\nsome unfortunate usage pattern.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1002
      },
      "id": 253,
      "targets": [
        {
          "expr": "rate(arangodb_v8_context_created_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total number of V8 contexts ever created",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This reflects the current number of callbacks the local `AgencyCache`\nhas registered.\nThis metric was named `arangodb_agency_cache_callback_count` in\nprevious versions of ArangoDB.\nNote that on single servers this metrics will only have a non-zero value\nin \"active failover\" deployment mode.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1002
      },
      "id": 263,
      "targets": [
        {
          "expr": "arangodb_agency_cache_callback_number",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Current number of entries in Agency cache callbacks table",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This counter reflects the total number of V8 contexts ever destroyed.\nIt is OK if this number keeps growing since the V8 contexts are\ncreated and destroyed as needed. In rare cases a high fluctuation can\nindicate some unfortunate usage pattern.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1010
      },
      "id": 255,
      "targets": [
        {
          "expr": "rate(arangodb_v8_context_destroyed_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total number of V8 contexts ever destroyed",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric was named `arangodb_agency_callback_registered` in previous versions\nof ArangoDB.\nNote that on single servers this metrics will only have a non-zero value\nin \"active failover\" deployment mode.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1010
      },
      "id": 265,
      "targets": [
        {
          "expr": "rate(arangodb_agency_callback_registered_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total number of Agency callbacks ever registered",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total number of V8 context enter events. A context receives a context\nenter event every time it begins to execute some JavaScript. This number\nis a rough estimate as to how much JavaScript the server executes.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1018
      },
      "id": 257,
      "targets": [
        {
          "expr": "rate(arangodb_v8_context_entered_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total number of V8 context enter events",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Agency RAFT commit time histogram. Provides a distribution\nof commit times for all Agency write operations.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1018
      },
      "id": 267,
      "legend": {
        "show": false
      },
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(arangodb_agency_commit_hist_bucket[3m])) by (le))",
          "format": "heatmap",
          "legendFormat": "",
          "refId": "A"
        }
      ],
      "title": "Agency RAFT commit histogram",
      "type": "heatmap"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 1026
      },
      "id": 259,
      "panels": [],
      "title": "Agency",
      "type": "row"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This measures the time an Agency follower needs for individual\nappend operations resulting from `AppendEntriesRPC` requests.\nEvery event contributes a measurement to the histogram, which\nalso exposes the number of events and the total sum of all\nmeasurements.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1027
      },
      "id": 260,
      "legend": {
        "show": false
      },
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(arangodb_agency_append_hist_bucket[3m])) by (le))",
          "format": "heatmap",
          "legendFormat": "",
          "refId": "A"
        }
      ],
      "title": "Agency RAFT follower append time histogram",
      "type": "heatmap"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This measures the time an Agency follower needs for individual\nappend operations resulting from `AppendEntriesRPC` requests.\nEvery event contributes a measurement to the histogram, which\nalso exposes the number of events and the total sum of all\nmeasurements.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1027
      },
      "id": 261,
      "targets": [
        {
          "expr": "rate(arangodb_agency_append_hist_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Agency RAFT follower append time histogram (count of events per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This measures the time an Agency follower needs for individual\nappend operations resulting from `AppendEntriesRPC` requests.\nEvery event contributes a measurement to the histogram, which\nalso exposes the number of events and the total sum of all\nmeasurements.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1035
      },
      "id": 262,
      "targets": [
        {
          "expr": "rate(arangodb_agency_append_hist_sum[3m]) / rate(arangodb_agency_append_hist_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Agency RAFT follower append time histogram (average per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Agency RAFT commit time histogram. Provides a distribution\nof commit times for all Agency write operations.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1035
      },
      "id": 269,
      "targets": [
        {
          "expr": "rate(arangodb_agency_commit_hist_sum[3m]) / rate(arangodb_agency_commit_hist_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Agency RAFT commit histogram (average per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This metric reflects the current number of Agency callbacks being\nregistered, including Agency cache callbacks.\nThis metric was named `arangodb_agency_callback_count` in previous versions\nof ArangoDB.\nNote that on single servers this metrics will only have a non-zero value\nin \"active failover\" deployment mode.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1043
      },
      "id": 264,
      "targets": [
        {
          "expr": "arangodb_agency_callback_number",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Current number of Agency callbacks registered",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Agency compaction time histogram. Provides a distribution\nof Agency compaction run times. Compactions are triggered after\n`--agency.compaction-keep-size` entries have accumulated in the\nRAFT log.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1043
      },
      "id": 271,
      "targets": [
        {
          "expr": "rate(arangodb_agency_compaction_hist_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Agency compaction time histogram (count of events per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Current number of entries in Agency client id lookup table.\nThe lookup table is used internally for Agency inquire operations\nand should be compacted at the same time when the Agency's in-memory\nlog is compacted.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1051
      },
      "id": 266,
      "targets": [
        {
          "expr": "arangodb_agency_client_lookup_table_size",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Current number of entries in Agency client id lookup table",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This Agent's commit index (i.e. the index until it has advanced in\nthe Agency's RAFT protocol).\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1051
      },
      "id": 273,
      "targets": [
        {
          "expr": "arangodb_agency_local_commit_index",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "This Agent's commit index",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Agency RAFT commit time histogram. Provides a distribution\nof commit times for all Agency write operations.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1059
      },
      "id": 268,
      "targets": [
        {
          "expr": "rate(arangodb_agency_commit_hist_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Agency RAFT commit histogram (count of events per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total number of Agency read operations with no leader or on followers.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1059
      },
      "id": 275,
      "targets": [
        {
          "expr": "rate(arangodb_agency_read_no_leader_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Agency read operations with no leader or on followers",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Agency compaction time histogram. Provides a distribution\nof Agency compaction run times. Compactions are triggered after\n`--agency.compaction-keep-size` entries have accumulated in the\nRAFT log.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1067
      },
      "id": 270,
      "legend": {
        "show": false
      },
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(arangodb_agency_compaction_hist_bucket[3m])) by (le))",
          "format": "heatmap",
          "legendFormat": "",
          "refId": "A"
        }
      ],
      "title": "Agency compaction time histogram",
      "type": "heatmap"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Counter for FailedServer jobs. This counter is increased whenever a\nsupervision run encounters a failed server and starts a FailedServer job.\nThis metric was named `arangodb_agency_supervision_failed_server_count`\nin previous versions of ArangoDB.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1067
      },
      "id": 277,
      "targets": [
        {
          "expr": "rate(arangodb_agency_supervision_failed_server_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Counter for FailedServer jobs",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Agency compaction time histogram. Provides a distribution\nof Agency compaction run times. Compactions are triggered after\n`--agency.compaction-keep-size` entries have accumulated in the\nRAFT log.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1075
      },
      "id": 272,
      "targets": [
        {
          "expr": "rate(arangodb_agency_compaction_hist_sum[3m]) / rate(arangodb_agency_compaction_hist_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Agency compaction time histogram (average per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Agency supervision runtime histogram. A new value is recorded for each\nrun of the supervision.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1075
      },
      "id": 279,
      "targets": [
        {
          "expr": "rate(arangodb_agency_supervision_runtime_msec_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Agency supervision runtime histogram (count of events per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Size of the Agency's in-memory part of replicated log in bytes.\nThe replicated log will grow in memory until a certain number of\nlog entries have been accumulated. Then the in-memory log will\nbe compacted. The number of in-memory log entries to keep before\nlog compaction kicks in can be controlled via the startup option\n`--agency.compaction-keep-size`.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1083
      },
      "id": 274,
      "targets": [
        {
          "expr": "arangodb_agency_log_size_bytes",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Agency replicated log size",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Agency supervision replication time histogram. Whenever the Agency\nsupervision carries out changes, it will write them to the leader's log\nand replicate the changes to followers. This metric provides a histogram\nof the time it took to replicate the supervision changes to followers.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1083
      },
      "id": 281,
      "legend": {
        "show": false
      },
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(arangodb_agency_supervision_runtime_wait_for_replication_msec_bucket[3m])) by (le))",
          "format": "heatmap",
          "legendFormat": "",
          "refId": "A"
        }
      ],
      "title": "Agency supervision wait for replication time",
      "type": "heatmap"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Number of Agency read operations which were successful (i.e. completed\nwithout any error). Successful reads can only be executed on the leader, so\nthis metric is supposed to increase only on Agency leaders, but not on\nfollowers. Read requests that are executed on followers will be rejected\nand can be tracked via the metric `arangodb_agency_read_no_leader_total`.\nThis metric was named `arangodb_agency_read_ok` in previous\nversions of ArangoDB.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1091
      },
      "id": 276,
      "targets": [
        {
          "expr": "rate(arangodb_agency_read_ok_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of successful Agency read operations",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Agency supervision replication time histogram. Whenever the Agency\nsupervision carries out changes, it will write them to the leader's log\nand replicate the changes to followers. This metric provides a histogram\nof the time it took to replicate the supervision changes to followers.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1091
      },
      "id": 283,
      "targets": [
        {
          "expr": "rate(arangodb_agency_supervision_runtime_wait_for_replication_msec_sum[3m]) / rate(arangodb_agency_supervision_runtime_wait_for_replication_msec_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Agency supervision wait for replication time (average per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Agency supervision runtime histogram. A new value is recorded for each\nrun of the supervision.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1099
      },
      "id": 278,
      "legend": {
        "show": false
      },
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(arangodb_agency_supervision_runtime_msec_bucket[3m])) by (le))",
          "format": "heatmap",
          "legendFormat": "",
          "refId": "A"
        }
      ],
      "title": "Agency supervision runtime histogram",
      "type": "heatmap"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Agency write time histogram. This histogram provides the distribution\nof the times spent in Agency write operations, in milliseconds. This only\nincludes the time required to write the data into the leader's log, but\ndoes not include the time required to replicate the writes to the followers.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1099
      },
      "id": 285,
      "legend": {
        "show": false
      },
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(arangodb_agency_write_hist_bucket[3m])) by (le))",
          "format": "heatmap",
          "legendFormat": "",
          "refId": "A"
        }
      ],
      "title": "Agency write time histogram",
      "type": "heatmap"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Agency supervision runtime histogram. A new value is recorded for each\nrun of the supervision.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1107
      },
      "id": 280,
      "targets": [
        {
          "expr": "rate(arangodb_agency_supervision_runtime_msec_sum[3m]) / rate(arangodb_agency_supervision_runtime_msec_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Agency supervision runtime histogram (average per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Agency write time histogram. This histogram provides the distribution\nof the times spent in Agency write operations, in milliseconds. This only\nincludes the time required to write the data into the leader's log, but\ndoes not include the time required to replicate the writes to the followers.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1107
      },
      "id": 287,
      "targets": [
        {
          "expr": "rate(arangodb_agency_write_hist_sum[3m]) / rate(arangodb_agency_write_hist_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Agency write time histogram (average per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Agency supervision replication time histogram. Whenever the Agency\nsupervision carries out changes, it will write them to the leader's log\nand replicate the changes to followers. This metric provides a histogram\nof the time it took to replicate the supervision changes to followers.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1115
      },
      "id": 282,
      "targets": [
        {
          "expr": "rate(arangodb_agency_supervision_runtime_wait_for_replication_msec_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Agency supervision wait for replication time (count of events per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Number of Agency write operations which were successful (i.e. completed\nwithout any error). Successful writes can only be executed on the leader, so\nthis metric is supposed to increase only on Agency leaders, but not on\nfollowers. Write requests that are executed on followers will be rejected\nand can be tracked via the metric `arangodb_agency_write_no_leader_total`.\nThis metric was named `arangodb_agency_write_ok` in previous\nversions of ArangoDB.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1115
      },
      "id": 289,
      "targets": [
        {
          "expr": "rate(arangodb_agency_write_ok_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of successful Agency write operations",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "The Agency's current term.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1123
      },
      "id": 284,
      "targets": [
        {
          "expr": "arangodb_agency_term",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Agency's term",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "The total number of queue jobs done.\nCalculating the difference between arangodb_scheduler_jobs_dequeued_total\nand arangodb_scheduler_jobs_done_total gives the total number of jobs\ncurrently being processed.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1123
      },
      "id": 294,
      "targets": [
        {
          "expr": "arangodb_scheduler_jobs_done_total",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total number of queue jobs done",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Agency write time histogram. This histogram provides the distribution\nof the times spent in Agency write operations, in milliseconds. This only\nincludes the time required to write the data into the leader's log, but\ndoes not include the time required to replicate the writes to the followers.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1131
      },
      "id": 286,
      "targets": [
        {
          "expr": "rate(arangodb_agency_write_hist_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Agency write time histogram (count of events per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Last recorded dequeue time for a low priority queue item, i.e., the amount of\ntime the job was sitting in the queue. If there is nothing to do for a long\ntime, this metric will be reset to zero.\nA large value for this metric indicates that the server is under heavy load\nand low priority jobs cannot be dequeued in a timely manner\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1131
      },
      "id": 296,
      "targets": [
        {
          "expr": "arangodb_scheduler_low_prio_queue_last_dequeue_time",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Last recorded dequeue time for a low priority queue item",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total number of Agency write operations with no leader or on followers.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1139
      },
      "id": 288,
      "targets": [
        {
          "expr": "rate(arangodb_agency_write_no_leader_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Agency write operations with no leader or on followers",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "The number of jobs currently queued on the scheduler's maintenance priority\nqueue. These are the jobs with the highest priority and are mainly used for\ncluster internal operations. The capacity of the maintenance priority queue\ncan be configured via the startup option `--server.scheduler-queue-size`.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1139
      },
      "id": 298,
      "targets": [
        {
          "expr": "arangodb_scheduler_maintenance_prio_queue_length",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Current queue length of the maintenance priority queue in the scheduler",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 1147
      },
      "id": 290,
      "panels": [],
      "title": "Scheduler",
      "type": "row"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total number of REST handler tasks that were created for execution\nvia the scheduler. This counter is increased for each incoming\nrequest for which a REST handler mapping exists and that does not\nneed to be forwarded to another coordinator in the cluster.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1148
      },
      "id": 291,
      "targets": [
        {
          "expr": "rate(arangodb_scheduler_handler_tasks_created_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total number of REST handler tasks created for the scheduler",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "The number of jobs currently queued on the scheduler's high priority queue.\nThe capacity of the high priority queue can be configured via the startup\noption `--server.prio1-size`.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1148
      },
      "id": 292,
      "targets": [
        {
          "expr": "arangodb_scheduler_high_prio_queue_length",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Current queue length of the high priority queue in the scheduler",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "The total number of jobs dequeued from all scheduler queues.\nCalculating the difference between arangodb_scheduler_jobs_submitted_total\nand arangodb_scheduler_jobs_dequeued_total gives the total number of\ncurrently queued jobs.\nCalculating the difference between arangodb_scheduler_jobs_dequeued_total\nand arangodb_scheduler_jobs_done_total gives the number of jobs currently\nbeing processed.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1156
      },
      "id": 293,
      "targets": [
        {
          "expr": "rate(arangodb_scheduler_jobs_dequeued_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total number of jobs dequeued",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "The number of worker threads currently working on some job or spinning while\nwaiting for new work (i.e., not sleeping).\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1156
      },
      "id": 300,
      "targets": [
        {
          "expr": "arangodb_scheduler_num_awake_threads",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of awake worker threads",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total number of jobs submitted to the scheduler.\nCalculating the difference between arangodb_scheduler_jobs_submitted_total\nand arangodb_scheduler_jobs_dequeued_total gives the total number of\ncurrently queued jobs.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1164
      },
      "id": 295,
      "targets": [
        {
          "expr": "arangodb_scheduler_jobs_submitted_total",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total number of jobs submitted to the scheduler",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "The current number of threads actually working on some job (i.e., not\nspinning while waiting for new work).\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1164
      },
      "id": 302,
      "targets": [
        {
          "expr": "arangodb_scheduler_num_working_threads",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Current number of working threads",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "The number of jobs currently queued on the scheduler's low priority queue.\nThe capacity of the low priority queue can be configured via the startup\noption `--server.maximal-queue-size`.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1172
      },
      "id": 297,
      "targets": [
        {
          "expr": "arangodb_scheduler_low_prio_queue_length",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Current queue length of the low priority queue in the scheduler",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Number of tasks dropped because the queue was already full. The queue capacities\ncan be configured via the startup options `--server.scheduler-queue-size`,\n`--server.prio1-size`, `--server.prio2-size` and `--server.maximal-queue-size`.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1172
      },
      "id": 304,
      "targets": [
        {
          "expr": "rate(arangodb_scheduler_queue_full_failures_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of tasks dropped and not added to internal queue",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "The number of jobs currently queued on the scheduler's medium priority queue.\nThe capacity of the medium priority queue can be configured via the startup\noption `--server.prio2-size`.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1180
      },
      "id": 299,
      "targets": [
        {
          "expr": "arangodb_scheduler_medium_prio_queue_length",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Current queue length of the medium priority queue in the scheduler",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Number of tasks/requests dropped because the client-specified queue time\nrequirements, as indicated by client applications in the request header\n\"x-arango-queue-time-seconds\" could not be satisfied by the receiving \nserver instance. This happens when the actual time need to queue/dequeue\nrequests on the scheduler queue exceeds the maximum time value that the\nclient has specified in the request. \nWhenever this happens, the client application will get an HTTP 412 error \nresponse back with error code 21004 (\"queue time violated\").\nAlthough the metric is exposed on all instance types, it will very likely\nalways be 0 on DB servers, simply because coordinators do not forward the\n\"x-arango-queue-time-seconds\" when they send internal requests to DB\nservers.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1180
      },
      "id": 306,
      "targets": [
        {
          "expr": "rate(arangodb_scheduler_queue_time_violations_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Number of tasks/requests dropped and not added to internal queue\ndue to the client-specified queue time requirements not being\nsatisfiable",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "The number of worker threads currently started. Worker threads can be started\nand stopped dynamically based on the server load.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1188
      },
      "id": 301,
      "targets": [
        {
          "expr": "arangodb_scheduler_num_worker_threads",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Current number of worker threads",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total accumulated number of scheduler threads stopped. Worker threads can be\nstarted and stopped dynamically based on the server load.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1188
      },
      "id": 308,
      "targets": [
        {
          "expr": "rate(arangodb_scheduler_threads_stopped_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Accumulated total number of scheduler threads stopped",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total number of low priority jobs currently being processed.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1196
      },
      "id": 303,
      "targets": [
        {
          "expr": "arangodb_scheduler_ongoing_low_prio",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total number of ongoing RestHandlers coming from the low priority queue",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of `Plan` loading runtimes, i.e. the runtimes\nof the `ClusterInfo::loadPlan` internal method. Provides a\ndistribution of all loading times for the `Plan`\nsection of the Agency data. The `Plan` section normally gets\nloaded on server startup, and then gets reloaded on servers\nonly for any databases in which there have been recent structural\nchanges (i.e. DDL changes).\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1196
      },
      "id": 313,
      "legend": {
        "show": false
      },
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(arangodb_load_plan_runtime_bucket[3m])) by (le))",
          "format": "heatmap",
          "legendFormat": "",
          "refId": "A"
        }
      ],
      "title": "`Plan` loading runtimes",
      "type": "heatmap"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "The total number of currently queued jobs in all queues.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1204
      },
      "id": 305,
      "targets": [
        {
          "expr": "arangodb_scheduler_queue_length",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Server's internal queue length",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of `Plan` loading runtimes, i.e. the runtimes\nof the `ClusterInfo::loadPlan` internal method. Provides a\ndistribution of all loading times for the `Plan`\nsection of the Agency data. The `Plan` section normally gets\nloaded on server startup, and then gets reloaded on servers\nonly for any databases in which there have been recent structural\nchanges (i.e. DDL changes).\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1204
      },
      "id": 315,
      "targets": [
        {
          "expr": "rate(arangodb_load_plan_runtime_sum[3m]) / rate(arangodb_load_plan_runtime_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "`Plan` loading runtimes (average per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Total accumulated number of scheduler threads started. Worker threads can be\nstarted and stopped dynamically based on the server load.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1212
      },
      "id": 307,
      "targets": [
        {
          "expr": "rate(arangodb_scheduler_threads_started_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total accumulated number of scheduler threads started",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Database servers execute reconciliation actions to let the cluster converge\nto the desired state. Actions are created, registered, queued and executed.\nOnce they are done they will eventually be removed.\n\nThis metric counts the number of actions that have been created but found to\nbe a duplicate of a already queued action.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1212
      },
      "id": 317,
      "targets": [
        {
          "expr": "rate(arangodb_maintenance_action_duplicate_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Counter of actions that have been discarded because of a duplicate",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 1220
      },
      "id": 309,
      "panels": [],
      "title": "Maintenance",
      "type": "row"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of `Current` loading runtimes, i.e. the runtimes\nof the `ClusterInfo::loadCurrent` internal method. Provides a\ndistribution of all loading times for the `Current`\nsection of the Agency data. The `Current` section gets\nloaded on server startup, and then gets reloaded on servers\nonly for any databases in which there have been recent structural\nchanges (i.e. DDL changes).\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1221
      },
      "id": 310,
      "legend": {
        "show": false
      },
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(arangodb_load_current_runtime_bucket[3m])) by (le))",
          "format": "heatmap",
          "legendFormat": "",
          "refId": "A"
        }
      ],
      "title": "`Current` loading runtimes",
      "type": "heatmap"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of `Current` loading runtimes, i.e. the runtimes\nof the `ClusterInfo::loadCurrent` internal method. Provides a\ndistribution of all loading times for the `Current`\nsection of the Agency data. The `Current` section gets\nloaded on server startup, and then gets reloaded on servers\nonly for any databases in which there have been recent structural\nchanges (i.e. DDL changes).\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1221
      },
      "id": 311,
      "targets": [
        {
          "expr": "rate(arangodb_load_current_runtime_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "`Current` loading runtimes (count of events per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of `Current` loading runtimes, i.e. the runtimes\nof the `ClusterInfo::loadCurrent` internal method. Provides a\ndistribution of all loading times for the `Current`\nsection of the Agency data. The `Current` section gets\nloaded on server startup, and then gets reloaded on servers\nonly for any databases in which there have been recent structural\nchanges (i.e. DDL changes).\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1229
      },
      "id": 312,
      "targets": [
        {
          "expr": "rate(arangodb_load_current_runtime_sum[3m]) / rate(arangodb_load_current_runtime_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "`Current` loading runtimes (average per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Database servers execute reconciliation actions to let the cluster converge\nto the desired state. Actions are created, registered, queued and executed.\nOnce they are done they will eventually be removed.\n\nThis metric tracks the time actions spend waiting in the queue in a histogram.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1229
      },
      "id": 319,
      "legend": {
        "show": false
      },
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(arangodb_maintenance_action_queue_time_msec_bucket[3m])) by (le))",
          "format": "heatmap",
          "legendFormat": "",
          "refId": "A"
        }
      ],
      "title": "Time spent in the queue before execution for maintenance actions",
      "type": "heatmap"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Histogram of `Plan` loading runtimes, i.e. the runtimes\nof the `ClusterInfo::loadPlan` internal method. Provides a\ndistribution of all loading times for the `Plan`\nsection of the Agency data. The `Plan` section normally gets\nloaded on server startup, and then gets reloaded on servers\nonly for any databases in which there have been recent structural\nchanges (i.e. DDL changes).\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1237
      },
      "id": 314,
      "targets": [
        {
          "expr": "rate(arangodb_load_plan_runtime_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "`Plan` loading runtimes (count of events per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Database servers execute reconciliation actions to let the cluster converge\nto the desired state. Actions are created, registered, queued and executed.\nOnce they are done they will eventually be removed.\n\nThis metric tracks the time actions spend waiting in the queue in a histogram.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1237
      },
      "id": 321,
      "targets": [
        {
          "expr": "rate(arangodb_maintenance_action_queue_time_msec_sum[3m]) / rate(arangodb_maintenance_action_queue_time_msec_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Time spent in the queue before execution for maintenance actions (average per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Database servers execute reconciliation actions to let the cluster converge\nto the desired state. Actions are created, registered, queued and executed.\nOnce they are done they will eventually be removed.\n\nThis metric counts the number of actions that are done and have been removed.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1245
      },
      "id": 316,
      "targets": [
        {
          "expr": "rate(arangodb_maintenance_action_done_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Counter of actions that are done and have been removed from the registry",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Database servers execute reconciliation actions to let the cluster converge\nto the desired state. Actions are created, registered, queued and executed.\nOnce they are done they will eventually be removed.\n\nThis metric tracks the time actions spend executing in a histogram.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1245
      },
      "id": 323,
      "legend": {
        "show": false
      },
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(arangodb_maintenance_action_runtime_msec_bucket[3m])) by (le))",
          "format": "heatmap",
          "legendFormat": "",
          "refId": "A"
        }
      ],
      "title": "Time spent executing a maintenance action",
      "type": "heatmap"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Database servers execute reconciliation actions to let the cluster converge\nto the desired state. Actions are created, registered, queued and executed.\nOnce they are done they will eventually be removed.\n\nThose action can fail for different reasons. This metric counts the failed\nactions and can thus provide hints to investigate a malfunction.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1253
      },
      "id": 318,
      "targets": [
        {
          "expr": "rate(arangodb_maintenance_action_failure_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Failure counter for the maintenance actions",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Database servers execute reconciliation actions to let the cluster converge\nto the desired state. Actions are created, registered, queued and executed.\nOnce they are done they will eventually be removed.\n\nThis metric tracks the time actions spend executing in a histogram.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1253
      },
      "id": 325,
      "targets": [
        {
          "expr": "rate(arangodb_maintenance_action_runtime_msec_sum[3m]) / rate(arangodb_maintenance_action_runtime_msec_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Time spent executing a maintenance action (average per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Database servers execute reconciliation actions to let the cluster converge\nto the desired state. Actions are created, registered, queued and executed.\nOnce they are done they will eventually be removed.\n\nThis metric tracks the time actions spend waiting in the queue in a histogram.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1261
      },
      "id": 320,
      "targets": [
        {
          "expr": "rate(arangodb_maintenance_action_queue_time_msec_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Time spent in the queue before execution for maintenance actions (count of events per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Database servers execute reconciliation actions to let the cluster converge\nto the desired state. To identify the target state differences in the meta\ndata store provided by the Agency are investigated and local changes are\nreported. This process is called Agency sync and is executed in regular\nintervals.\n\nThis metric tracks the runtime of individual Agency syncs in a histogram.\nDuring DDL operations the runtime can increase but should generally be below\n1s.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1261
      },
      "id": 327,
      "targets": [
        {
          "expr": "rate(arangodb_maintenance_agency_sync_runtime_msec_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total time spent on Agency sync (count of events per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Database servers execute reconciliation actions to let the cluster converge\nto the desired state. Actions are created, registered, queued and executed.\nOnce they are done they will eventually be removed.\n\nThis metric counts the number of actions that are queued or active.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1269
      },
      "id": 322,
      "targets": [
        {
          "expr": "rate(arangodb_maintenance_action_registered_total[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Counter of actions that have been registered in the action registry",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Database servers execute reconciliation actions to let the cluster converge\nto the desired state. To identify the target state differences in the meta\ndata store provided by the Agency are investigated and local changes are\nreported. This process is called Agency sync and is executed in regular\nintervals.\n\nThis metric tracks the runtime of phase1 of an Agency sync. Phase1 calculates\nthe difference between the local and the target state.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1269
      },
      "id": 329,
      "legend": {
        "show": false
      },
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(arangodb_maintenance_phase1_runtime_msec_bucket[3m])) by (le))",
          "format": "heatmap",
          "legendFormat": "",
          "refId": "A"
        }
      ],
      "title": "Maintenance Phase 1 runtime histogram",
      "type": "heatmap"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Database servers execute reconciliation actions to let the cluster converge\nto the desired state. Actions are created, registered, queued and executed.\nOnce they are done they will eventually be removed.\n\nThis metric tracks the time actions spend executing in a histogram.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1277
      },
      "id": 324,
      "targets": [
        {
          "expr": "rate(arangodb_maintenance_action_runtime_msec_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Time spent executing a maintenance action (count of events per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Database servers execute reconciliation actions to let the cluster converge\nto the desired state. To identify the target state differences in the meta\ndata store provided by the Agency are investigated and local changes are\nreported. This process is called Agency sync and is executed in regular\nintervals.\n\nThis metric tracks the runtime of phase1 of an Agency sync. Phase1 calculates\nthe difference between the local and the target state.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1277
      },
      "id": 331,
      "targets": [
        {
          "expr": "rate(arangodb_maintenance_phase1_runtime_msec_sum[3m]) / rate(arangodb_maintenance_phase1_runtime_msec_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Maintenance Phase 1 runtime histogram (average per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Database servers execute reconciliation actions to let the cluster converge\nto the desired state. To identify the target state differences in the meta\ndata store provided by the Agency are investigated and local changes are\nreported. This process is called Agency sync and is executed in regular\nintervals.\n\nThis metric tracks the runtime of individual Agency syncs in a histogram.\nDuring DDL operations the runtime can increase but should generally be below\n1s.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1285
      },
      "id": 326,
      "legend": {
        "show": false
      },
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(arangodb_maintenance_agency_sync_runtime_msec_bucket[3m])) by (le))",
          "format": "heatmap",
          "legendFormat": "",
          "refId": "A"
        }
      ],
      "title": "Total time spent on Agency sync",
      "type": "heatmap"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Database servers execute reconciliation actions to let the cluster converge\nto the desired state. To identify the target state differences in the meta\ndata store provided by the Agency are investigated and local changes are\nreported. This process is called Agency sync and is executed in regular\nintervals.\n\nThis metric tracks the runtime of phase2 of an Agency sync. Phase2 calculates\nwhat actions to execute given the difference of the local and target state.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 1285
      },
      "id": 333,
      "targets": [
        {
          "expr": "rate(arangodb_maintenance_phase2_runtime_msec_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Maintenance Phase 2 runtime histogram (count of events per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Database servers execute reconciliation actions to let the cluster converge\nto the desired state. To identify the target state differences in the meta\ndata store provided by the Agency are investigated and local changes are\nreported. This process is called Agency sync and is executed in regular\nintervals.\n\nThis metric tracks the runtime of individual Agency syncs in a histogram.\nDuring DDL operations the runtime can increase but should generally be below\n1s.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1293
      },
      "id": 328,
      "targets": [
        {
          "expr": "rate(arangodb_maintenance_agency_sync_runtime_msec_sum[3m]) / rate(arangodb_maintenance_agency_sync_runtime_msec_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Total time spent on Agency sync (average per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Database servers execute reconciliation actions to let the cluster converge\nto the desired state. To identify the target state differences in the meta\ndata store provided by the Agency are investigated and local changes are\nreported. This process is called Agency sync and is executed in regular\nintervals.\n\nThis metric tracks the runtime of phase1 of an Agency sync. Phase1 calculates\nthe difference between the local and the target state.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1301
      },
      "id": 330,
      "targets": [
        {
          "expr": "rate(arangodb_maintenance_phase1_runtime_msec_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Maintenance Phase 1 runtime histogram (count of events per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Database servers execute reconciliation actions to let the cluster converge\nto the desired state. To identify the target state differences in the meta\ndata store provided by the Agency are investigated and local changes are\nreported. This process is called Agency sync and is executed in regular\nintervals.\n\nThis metric tracks the runtime of phase2 of an Agency sync. Phase2 calculates\nwhat actions to execute given the difference of the local and target state.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1309
      },
      "id": 332,
      "legend": {
        "show": false
      },
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(arangodb_maintenance_phase2_runtime_msec_bucket[3m])) by (le))",
          "format": "heatmap",
          "legendFormat": "",
          "refId": "A"
        }
      ],
      "title": "Maintenance Phase 2 runtime histogram",
      "type": "heatmap"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "Database servers execute reconciliation actions to let the cluster converge\nto the desired state. To identify the target state differences in the meta\ndata store provided by the Agency are investigated and local changes are\nreported. This process is called Agency sync and is executed in regular\nintervals.\n\nThis metric tracks the runtime of phase2 of an Agency sync. Phase2 calculates\nwhat actions to execute given the difference of the local and target state.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1317
      },
      "id": 334,
      "targets": [
        {
          "expr": "rate(arangodb_maintenance_phase2_runtime_msec_sum[3m]) / rate(arangodb_maintenance_phase2_runtime_msec_count[3m])",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "Maintenance Phase 2 runtime histogram (average per second)",
      "type": "graph"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 1325
      },
      "id": 335,
      "panels": [],
      "title": "License",
      "type": "row"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "YcPg057nk"
      },
      "description": "This instance's remaining license validity time.\n",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 1326
      },
      "id": 336,
      "targets": [
        {
          "expr": "arangodb_license_expires",
          "legendFormat": "{{instance}}:{{shortname}}",
          "refId": "A"
        }
      ],
      "title": "This instance's license expiry in days",
      "type": "graph"
    }
  ],
  "schemaVersion": 36,
  "style": "dark",
  "tags": [],
  "templating": {
    "list": []
  },
  "time": {
    "from": "now-6h",
    "to": "now"
  },
  "timepicker": {},
  "timezone": "",
  "title": "ArangoDB_3.9_for_arangodbdevelopers",
  "uid": "kN6KDdl7z",
  "version": 1,
  "weekStart": ""
}